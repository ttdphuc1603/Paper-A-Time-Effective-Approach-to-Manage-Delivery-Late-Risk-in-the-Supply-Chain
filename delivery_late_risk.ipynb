{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  AdaBoost  GradientBoosting  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost  LightGBM  CatBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost  LightGBM  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  LightGBM  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "#  GradientBoosting  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost  LightGBM \n",
    "#  AdaBoost  GradientBoosting  XGBoost  CatBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  LightGBM  CatBoost \n",
    "#  AdaBoost  GradientBoosting  LightGBM  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  XGBoost  LightGBM  CatBoost \n",
    "#  AdaBoost  XGBoost  LightGBM  HistGradientBoost \n",
    "#  AdaBoost  XGBoost  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "#  GradientBoosting  XGBoost  LightGBM  CatBoost \n",
    "#  GradientBoosting  XGBoost  LightGBM  HistGradientBoost \n",
    "#  GradientBoosting  XGBoost  CatBoost  HistGradientBoost \n",
    "#  GradientBoosting  LightGBM  CatBoost  HistGradientBoost \n",
    "#  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting  XGBoost \n",
    "#  AdaBoost  GradientBoosting  LightGBM \n",
    "#  AdaBoost  GradientBoosting  CatBoost \n",
    "#  AdaBoost  GradientBoosting  HistGradientBoost \n",
    "#  AdaBoost  XGBoost  LightGBM \n",
    "#  AdaBoost  XGBoost  CatBoost \n",
    "#  AdaBoost  XGBoost  HistGradientBoost \n",
    "#  AdaBoost  LightGBM  CatBoost \n",
    "#  AdaBoost  LightGBM  HistGradientBoost \n",
    "#  AdaBoost  CatBoost  HistGradientBoost \n",
    "#  GradientBoosting  XGBoost  LightGBM \n",
    "#  GradientBoosting  XGBoost  CatBoost \n",
    "#  GradientBoosting  XGBoost  HistGradientBoost \n",
    "#  GradientBoosting  LightGBM  CatBoost \n",
    "#  GradientBoosting  LightGBM  HistGradientBoost \n",
    "#  GradientBoosting  CatBoost  HistGradientBoost \n",
    "#  XGBoost  LightGBM  CatBoost \n",
    "#  XGBoost  LightGBM  HistGradientBoost \n",
    "#  XGBoost  CatBoost  HistGradientBoost \n",
    "#  LightGBM  CatBoost  HistGradientBoost \n",
    "#  AdaBoost  GradientBoosting \n",
    "#  AdaBoost  XGBoost \n",
    "#  AdaBoost  LightGBM \n",
    "#  AdaBoost  CatBoost \n",
    "#  AdaBoost  HistGradientBoost \n",
    "#  GradientBoosting  XGBoost \n",
    "#  GradientBoosting  LightGBM \n",
    "#  GradientBoosting  CatBoost \n",
    "#  GradientBoosting  HistGradientBoost \n",
    "#  XGBoost  LightGBM \n",
    "#  XGBoost  CatBoost \n",
    "#  XGBoost  HistGradientBoost \n",
    "#  LightGBM  CatBoost \n",
    "#  LightGBM  HistGradientBoost \n",
    "#  CatBoost  HistGradientBoost \n",
    "#  AdaBoost \n",
    "#  GradientBoosting \n",
    "#  XGBoost \n",
    "#  LightGBM \n",
    "#  CatBoost \n",
    "#  HistGradientBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca02c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11f0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"figure.labelsize\"] =12\n",
    "plt.rcParams[\"font.serif\"] = 'Utopia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fbf21",
   "metadata": {},
   "source": [
    "DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e38ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Days for shipping (real)</th>\n",
       "      <th>Days for shipment (scheduled)</th>\n",
       "      <th>Benefit per order</th>\n",
       "      <th>Sales per customer</th>\n",
       "      <th>Delivery Status</th>\n",
       "      <th>Late_delivery_risk</th>\n",
       "      <th>Category Id</th>\n",
       "      <th>Category Name</th>\n",
       "      <th>Customer City</th>\n",
       "      <th>...</th>\n",
       "      <th>Order Zipcode</th>\n",
       "      <th>Product Card Id</th>\n",
       "      <th>Product Category Id</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Image</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Status</th>\n",
       "      <th>shipping date (DateOrders)</th>\n",
       "      <th>Shipping Mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>314.640015</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2/3/2018 22:56</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-249.089996</td>\n",
       "      <td>311.359985</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>1/18/2018 12:27</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASH</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-247.779999</td>\n",
       "      <td>309.720001</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>1/17/2018 12:06</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>22.860001</td>\n",
       "      <td>304.809998</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>1/16/2018 11:45</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>134.210007</td>\n",
       "      <td>298.250000</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>1/15/2018 11:24</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180514</th>\n",
       "      <td>CASH</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Field+%26+Stre...</td>\n",
       "      <td>Field &amp; Stream Sportsman 16 Gun Fire Safe</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>0</td>\n",
       "      <td>1/20/2016 3:40</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180515</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-613.770019</td>\n",
       "      <td>395.980011</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Bakersfield</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Field+%26+Stre...</td>\n",
       "      <td>Field &amp; Stream Sportsman 16 Gun Fire Safe</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>0</td>\n",
       "      <td>1/19/2016 1:34</td>\n",
       "      <td>Second Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180516</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>141.110001</td>\n",
       "      <td>391.980011</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Field+%26+Stre...</td>\n",
       "      <td>Field &amp; Stream Sportsman 16 Gun Fire Safe</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>0</td>\n",
       "      <td>1/20/2016 21:00</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180517</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>186.229996</td>\n",
       "      <td>387.980011</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Field+%26+Stre...</td>\n",
       "      <td>Field &amp; Stream Sportsman 16 Gun Fire Safe</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>0</td>\n",
       "      <td>1/18/2016 20:18</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180518</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>168.949997</td>\n",
       "      <td>383.980011</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Field+%26+Stre...</td>\n",
       "      <td>Field &amp; Stream Sportsman 16 Gun Fire Safe</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>0</td>\n",
       "      <td>1/19/2016 18:54</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180519 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Days for shipping (real)  Days for shipment (scheduled)  \\\n",
       "0          DEBIT                         3                              4   \n",
       "1       TRANSFER                         5                              4   \n",
       "2           CASH                         4                              4   \n",
       "3          DEBIT                         3                              4   \n",
       "4        PAYMENT                         2                              4   \n",
       "...          ...                       ...                            ...   \n",
       "180514      CASH                         4                              4   \n",
       "180515     DEBIT                         3                              2   \n",
       "180516  TRANSFER                         5                              4   \n",
       "180517   PAYMENT                         3                              4   \n",
       "180518   PAYMENT                         4                              4   \n",
       "\n",
       "        Benefit per order  Sales per customer   Delivery Status  \\\n",
       "0               91.250000          314.640015  Advance shipping   \n",
       "1             -249.089996          311.359985     Late delivery   \n",
       "2             -247.779999          309.720001  Shipping on time   \n",
       "3               22.860001          304.809998  Advance shipping   \n",
       "4              134.210007          298.250000  Advance shipping   \n",
       "...                   ...                 ...               ...   \n",
       "180514          40.000000          399.980011  Shipping on time   \n",
       "180515        -613.770019          395.980011     Late delivery   \n",
       "180516         141.110001          391.980011     Late delivery   \n",
       "180517         186.229996          387.980011  Advance shipping   \n",
       "180518         168.949997          383.980011  Shipping on time   \n",
       "\n",
       "        Late_delivery_risk  Category Id   Category Name Customer City  ...  \\\n",
       "0                        0           73  Sporting Goods        Caguas  ...   \n",
       "1                        1           73  Sporting Goods        Caguas  ...   \n",
       "2                        0           73  Sporting Goods      San Jose  ...   \n",
       "3                        0           73  Sporting Goods   Los Angeles  ...   \n",
       "4                        0           73  Sporting Goods        Caguas  ...   \n",
       "...                    ...          ...             ...           ...  ...   \n",
       "180514                   0           45         Fishing      Brooklyn  ...   \n",
       "180515                   1           45         Fishing   Bakersfield  ...   \n",
       "180516                   1           45         Fishing       Bristol  ...   \n",
       "180517                   0           45         Fishing        Caguas  ...   \n",
       "180518                   0           45         Fishing        Caguas  ...   \n",
       "\n",
       "       Order Zipcode Product Card Id Product Category Id  Product Description  \\\n",
       "0                NaN            1360                  73                  NaN   \n",
       "1                NaN            1360                  73                  NaN   \n",
       "2                NaN            1360                  73                  NaN   \n",
       "3                NaN            1360                  73                  NaN   \n",
       "4                NaN            1360                  73                  NaN   \n",
       "...              ...             ...                 ...                  ...   \n",
       "180514           NaN            1004                  45                  NaN   \n",
       "180515           NaN            1004                  45                  NaN   \n",
       "180516           NaN            1004                  45                  NaN   \n",
       "180517           NaN            1004                  45                  NaN   \n",
       "180518           NaN            1004                  45                  NaN   \n",
       "\n",
       "                                            Product Image  \\\n",
       "0            http://images.acmesports.sports/Smart+watch    \n",
       "1            http://images.acmesports.sports/Smart+watch    \n",
       "2            http://images.acmesports.sports/Smart+watch    \n",
       "3            http://images.acmesports.sports/Smart+watch    \n",
       "4            http://images.acmesports.sports/Smart+watch    \n",
       "...                                                   ...   \n",
       "180514  http://images.acmesports.sports/Field+%26+Stre...   \n",
       "180515  http://images.acmesports.sports/Field+%26+Stre...   \n",
       "180516  http://images.acmesports.sports/Field+%26+Stre...   \n",
       "180517  http://images.acmesports.sports/Field+%26+Stre...   \n",
       "180518  http://images.acmesports.sports/Field+%26+Stre...   \n",
       "\n",
       "                                     Product Name Product Price  \\\n",
       "0                                    Smart watch     327.750000   \n",
       "1                                    Smart watch     327.750000   \n",
       "2                                    Smart watch     327.750000   \n",
       "3                                    Smart watch     327.750000   \n",
       "4                                    Smart watch     327.750000   \n",
       "...                                           ...           ...   \n",
       "180514  Field & Stream Sportsman 16 Gun Fire Safe    399.980011   \n",
       "180515  Field & Stream Sportsman 16 Gun Fire Safe    399.980011   \n",
       "180516  Field & Stream Sportsman 16 Gun Fire Safe    399.980011   \n",
       "180517  Field & Stream Sportsman 16 Gun Fire Safe    399.980011   \n",
       "180518  Field & Stream Sportsman 16 Gun Fire Safe    399.980011   \n",
       "\n",
       "       Product Status shipping date (DateOrders)   Shipping Mode  \n",
       "0                   0             2/3/2018 22:56  Standard Class  \n",
       "1                   0            1/18/2018 12:27  Standard Class  \n",
       "2                   0            1/17/2018 12:06  Standard Class  \n",
       "3                   0            1/16/2018 11:45  Standard Class  \n",
       "4                   0            1/15/2018 11:24  Standard Class  \n",
       "...               ...                        ...             ...  \n",
       "180514              0             1/20/2016 3:40  Standard Class  \n",
       "180515              0             1/19/2016 1:34    Second Class  \n",
       "180516              0            1/20/2016 21:00  Standard Class  \n",
       "180517              0            1/18/2016 20:18  Standard Class  \n",
       "180518              0            1/19/2016 18:54  Standard Class  \n",
       "\n",
       "[180519 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"DataCoSupplyChainDataset.csv\", encoding='ISO-8859-1')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecc8e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late_delivery_risk      0      1\n",
      "Delivery Status                 \n",
      "Advance shipping    41592      0\n",
      "Late delivery           0  98977\n",
      "Shipping canceled    7754      0\n",
      "Shipping on time    32196      0\n"
     ]
    }
   ],
   "source": [
    "table = pd.crosstab(df['Delivery Status'], df['Late_delivery_risk'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0079ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days for shipping (real)     0     1      2      3      4      5      6\n",
      "Late_delivery_risk                                                     \n",
      "0                         5080   203  30105  22006  21754   1160   1234\n",
      "1                            0  4454  26513   6759   6759  27003  27489\n"
     ]
    }
   ],
   "source": [
    "table = pd.crosstab(df['Late_delivery_risk'], df['Days for shipping (real)'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0907eb",
   "metadata": {},
   "source": [
    "DATA TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea17413",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "for col in df:\n",
    "  # Check if the data type is object (string) for categorical data\n",
    "  if pd.api.types.is_string_dtype(df[col]):  # For pandas Series\n",
    "  # if isinstance(data_list[col], str):  # For raw list\n",
    "    category_cols.append(col)\n",
    "  else:\n",
    "    numerical_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2f0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = ['Delivery Status', 'Category Name', 'Customer City', 'Customer Country',\n",
    "                'Customer Email', 'Customer Fname', 'Customer Password', 'Customer Segment',\n",
    "                'Customer State', 'Customer Street', 'Department Name', 'Market', 'Order City',\n",
    "                'Order Country', 'order date (DateOrders)', 'Order Region','Order State',\n",
    "                'Order Status', 'Product Image', 'Product Name','shipping date (DateOrders)','Shipping Mode']\n",
    "\n",
    "numerical_features = ['Days for shipping (real)', 'Days for shipment (scheduled)', 'Benefit per order', 'Sales per customer',\n",
    "                'Late_delivery_risk', 'Category Id', 'Customer Id', 'Customer Lname', 'Customer Zipcode',\n",
    "                'Department Id', 'Latitude', 'Longitude', 'Order Customer Id', 'Order Id', 'Order Item Cardprod Id',\n",
    "                'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id', 'Order Item Product Price', 'Order Item Profit Ratio',\n",
    "                'Order Item Quantity', 'Sales', 'Order Item Total', 'Order Profit Per Order', 'Order Zipcode', 'Product Card Id',\n",
    "                'Product Category Id', 'Product Description', 'Product Price', 'Product Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07005176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery Status\n",
      "['Advance shipping' 'Late delivery' 'Shipping on time' 'Shipping canceled']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Category Name\n",
      "['Sporting Goods' 'Cleats' 'Shop By Sport' \"Women's Apparel\" 'Electronics'\n",
      " 'Boxing & MMA' 'Cardio Equipment' 'Trade-In' \"Kids' Golf Clubs\"\n",
      " 'Hunting & Shooting' 'Baseball & Softball' \"Men's Footwear\"\n",
      " 'Camping & Hiking' 'Consumer Electronics' 'Cameras ' 'Computers'\n",
      " 'Basketball' 'Soccer' \"Girls' Apparel\" 'Accessories' \"Women's Clothing\"\n",
      " 'Crafts' \"Men's Clothing\" 'Tennis & Racquet' 'Fitness Accessories'\n",
      " 'As Seen on  TV!' 'Golf Balls' 'Strength Training' \"Children's Clothing\"\n",
      " 'Lacrosse' 'Baby ' 'Fishing' 'Books ' 'DVDs' 'CDs ' 'Garden' 'Hockey'\n",
      " 'Pet Supplies' 'Health and Beauty' 'Music' 'Video Games' 'Golf Gloves'\n",
      " 'Golf Bags & Carts' 'Golf Shoes' 'Golf Apparel' \"Women's Golf Clubs\"\n",
      " \"Men's Golf Clubs\" 'Toys' 'Water Sports' 'Indoor/Outdoor Games']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer City\n",
      "['Caguas' 'San Jose' 'Los Angeles' 'Tonawanda' 'Miami' 'San Ramon'\n",
      " 'Freeport' 'Salinas' 'Peabody' 'Canovanas' 'Paramount' 'Mount Prospect'\n",
      " 'Long Beach' 'Rancho Cordova' 'Billings' 'Wilkes Barre' 'Roseville'\n",
      " 'Bellflower' 'Wheaton' 'Detroit' 'Dallas' 'Carlisle' 'Newark'\n",
      " 'Panorama City' 'Atlanta' 'Fremont' 'Rochester' 'Bayamon' 'Guayama'\n",
      " 'Juana Diaz' 'Fort Washington' 'Bakersfield' 'Corona' 'Cincinnati'\n",
      " 'Germantown' 'Carrollton' 'Houston' 'Ewa Beach' 'Lakewood' 'Rome' 'Vista'\n",
      " 'Fort Worth' 'Fond Du Lac' 'Philadelphia' 'Ontario' 'Oviedo' 'Buffalo'\n",
      " 'Honolulu' 'Oceanside' 'North Tonawanda' 'Clovis' 'Jamaica'\n",
      " 'Granite City' 'Medford' 'Pomona' 'Tempe' 'Santa Ana' 'York' 'Aurora'\n",
      " 'Simi Valley' 'Silver Spring' 'Saint Paul' 'San Antonio' 'Bronx'\n",
      " 'Greenville' 'Morristown' 'San Diego' 'Oxnard' 'Albuquerque' 'Amarillo'\n",
      " 'Lutz' 'Bend' 'East Brunswick' 'Lancaster' 'Hampton' 'New York'\n",
      " 'Porterville' 'Portland' 'Strongsville' 'El Paso' 'Del Rio' 'Bountiful'\n",
      " 'Kent' 'Chicago' 'Plymouth' 'Far Rockaway' 'Garden Grove' 'Placentia'\n",
      " 'Mentor' 'Santa Clara' 'Union' 'Westminster' 'Pompano Beach' 'Azusa'\n",
      " 'Fort Lauderdale' 'Princeton' 'Perth Amboy' 'Loveland' 'Virginia Beach'\n",
      " 'Louisville' 'Lockport' 'Staten Island' 'Tucson' 'Cleveland' 'Webster'\n",
      " 'Stockton' 'Martinsburg' 'Cumberland' 'Pekin' 'Tallahassee'\n",
      " 'Jacksonville' 'Woonsocket' 'Lithonia' 'Oak Lawn' 'Alhambra' 'New Haven'\n",
      " 'Phoenix' 'Kenner' 'Washington' 'Holland' 'Morrisville' 'Memphis'\n",
      " 'Federal Way' 'West Covina' 'Ventura' 'Valrico' 'Kaneohe' 'Brooklyn'\n",
      " 'Lodi' 'Murfreesboro' 'Carlsbad' 'Hamilton' 'Hayward' 'Bridgeton'\n",
      " 'Bay Shore' 'Palatine' 'Smyrna' 'Van Nuys' 'Opa Locka' 'Edison' 'Baytown'\n",
      " 'Sylmar' 'Burnsville' 'Huntington Station' 'Sunnyvale' 'Sugar Land'\n",
      " 'Brighton' 'Bismarck' 'Gaithersburg' 'Lilburn' 'Provo' 'Columbia'\n",
      " 'Marietta' 'Rio Grande' 'Denver' 'Taylor' 'Saint Charles' 'Cupertino'\n",
      " 'Springfield' 'Mission Viejo' 'Roswell' 'Ypsilanti' 'Peoria' 'Clementon'\n",
      " 'Antioch' 'Salt Lake City' 'Granada Hills' 'Hempstead' 'Astoria' 'Gilroy'\n",
      " 'Lenoir' 'Columbus' 'Albany' 'Humacao' 'Lindenhurst' 'Elyria' 'Riverside'\n",
      " 'Carson' 'Mesa' 'San Juan' 'Vega Baja' 'Mayaguez' 'Arecibo'\n",
      " 'San Sebastian' 'Eugene' 'Algonquin' 'Indianapolis' 'Buena Park'\n",
      " 'Catonsville' 'Jersey City' 'Lombard' 'New Bedford' 'Newburgh' 'Lansdale'\n",
      " 'Baltimore' 'Fullerton' 'Sacramento' 'Greensboro' 'Roseburg' 'Modesto'\n",
      " 'Encinitas' 'Watsonville' 'Meridian' 'Endicott' 'Katy' 'Visalia' 'Lompoc'\n",
      " 'Ogden' 'Raleigh' 'Hacienda Heights' 'Union City' 'Hollywood'\n",
      " 'Bolingbrook' 'West Lafayette' 'Woodbridge' 'Weslaco' 'Bell Gardens'\n",
      " 'La Mirada' 'North Bergen' 'Madison' 'South San Francisco'\n",
      " 'North Las Vegas' 'Methuen' 'Costa Mesa' 'Glen Burnie' 'Fairfield'\n",
      " 'Winnetka' 'Mcallen' 'Joliet' 'Brownsville' 'Pawtucket'\n",
      " 'Colorado Springs' 'Quincy' 'Pittsfield' 'Chino' 'Marion' 'North Hills'\n",
      " 'Salina' 'Hyattsville' 'North Richland Hills' 'Spring Valley' 'Lawrence'\n",
      " 'Milpitas' 'Rowland Heights' 'Gardena' 'Cicero' 'Asheboro' 'La Crosse'\n",
      " 'Florissant' 'Canyon Country' 'Ithaca' 'Allentown' 'Escondido' 'Martinez'\n",
      " 'Troy' 'Arlington' 'Davis' 'Chandler' 'Elgin' 'Palmdale' 'Massapequa'\n",
      " 'Pittsburg' 'West New York' 'Orlando' 'Hanover' 'Glendale' 'Enfield'\n",
      " 'Baldwin Park' 'Chino Hills' 'Toms River' 'Wyandotte' 'Mililani' 'Harvey'\n",
      " 'Mechanicsburg' 'Opelousas' 'Kailua' 'Norfolk' 'Elmhurst' 'Chillicothe'\n",
      " 'Canoga Park' 'Jackson' 'Moreno Valley' 'New Orleans' 'San Benito'\n",
      " 'New Castle' 'Bloomfield' 'Cypress' 'Marrero' 'Grand Prairie' 'Greeley'\n",
      " 'Littleton' 'Longmont' 'Chesapeake' 'Englewood' 'Arlington Heights'\n",
      " 'Tampa' 'Irvington' 'Forest Hills' 'Dearborn' 'Compton' 'Garland'\n",
      " 'Waipahu' 'Carmichael' 'Tustin' 'Anaheim' 'Canton' 'Stafford'\n",
      " 'South Richmond Hill' 'Middletown' 'West Orange' 'Daly City'\n",
      " 'Powder Springs' 'Parkville' 'Hialeah' 'Beloit' 'Aguadilla' 'Carolina'\n",
      " 'Yauco' 'Saint Peters' 'Augusta' 'Chapel Hill' 'East Lansing' 'Stamford'\n",
      " 'Diamond Bar' 'Milwaukee' 'Lawrenceville' 'Manchester' 'La Puente'\n",
      " 'Victorville' 'Richmond' 'Eagle Pass' 'Fontana' 'Ballwin' 'New Braunfels'\n",
      " 'Las Vegas' 'Goose Creek' 'Pharr' 'Yonkers' 'El Monte' 'Reynoldsburg'\n",
      " 'Hamtramck' 'Medina' 'Highland' 'Jonesboro' 'Elk Grove' 'Montebello'\n",
      " 'San Francisco' 'Glenview' 'Rock Hill' 'Austin' 'Scottsdale' 'Santa Cruz'\n",
      " 'Oregon City' 'Annandale' 'Plano' 'Piscataway' 'El Cajon' 'Hilliard'\n",
      " 'Orange Park' 'Decatur' 'San Pablo' 'Douglasville' 'Henderson'\n",
      " 'College Station' 'Round Rock' 'Mesquite' 'Broken Arrow' 'Redmond'\n",
      " 'Findlay' 'La Habra' 'Laguna Hills' 'San Bernardino' 'Apex'\n",
      " 'South El Monte' 'Irving' 'Blacksburg' 'Dorchester Center' 'Potomac'\n",
      " 'Winter Park' 'Stone Mountain' 'Goleta' 'Hagerstown' 'Alameda'\n",
      " 'Saint Louis' 'Pico Rivera' 'Chula Vista' 'Hollister' 'North Hollywood'\n",
      " 'New Brunswick' 'Beaverton' 'Chicago Heights' 'Hesperia' 'Cary' 'Sanford'\n",
      " 'Laredo' 'Westland' 'Stockbridge' 'Carol Stream' 'Wichita' 'Olathe'\n",
      " 'Flushing' 'Lynwood' 'Revere' 'Westerville' 'Cordova' 'Hanford' 'Rialto'\n",
      " 'Mchenry' 'Mission' 'Salem' 'Duluth' 'Danbury' 'Frankfort' 'Upland'\n",
      " 'Rosemead' 'Mount Pleasant' 'Lake Forest' 'West Chester' 'Woodside'\n",
      " 'Norcross' 'Fresno' 'Zanesville' 'Painesville' 'Lynnwood' 'Massillon'\n",
      " 'Crystal Lake' 'Rego Park' 'Ann Arbor' 'Wyoming' 'La Mesa' 'Edinburg'\n",
      " 'Howell' 'Michigan City' 'Sheboygan' 'Moline' 'Yuma' 'Campbell'\n",
      " 'Charlotte' 'Oakland' 'San Marcos' 'Walnut' 'Harlingen' 'Rio Rancho'\n",
      " 'Nashville' 'Annapolis' 'Laguna Niguel' 'Santee' 'West Jordan' 'Hickory'\n",
      " 'Manati' 'Trujillo Alto' 'Ponce' 'Toa Alta' 'Irwin' 'South Ozone Park'\n",
      " 'Ridgewood' 'Bowling Green' 'Richardson' 'Sun Valley' 'Huntington Beach'\n",
      " 'Fargo' 'Waukegan' 'Highland Park' 'Cerritos' 'Lewisville' 'Alpharetta'\n",
      " 'New Albany' 'Denton' 'Temecula' 'Tinley Park' 'Dundalk' 'Crown Point'\n",
      " 'Lawton' 'Fayetteville' 'Milford' 'Bartlett' 'Reno' 'Passaic' 'Reseda'\n",
      " 'Levittown' 'Wayne' 'Metairie' 'Wheeling' 'Hawthorne' 'Napa' 'Berwyn'\n",
      " 'Fountain Valley' 'Las Cruces' 'Apopka' 'Folsom' 'El Centro'\n",
      " 'Jackson Heights' 'Pacoima' 'Hendersonville' 'Clearfield' 'Seattle'\n",
      " 'Saginaw' 'Conway' 'Sandusky' 'San Pedro' 'Grove City' 'Knoxville'\n",
      " 'Huntington Park' 'Greensburg' 'Poway' 'O Fallon' 'Chambersburg' 'Normal'\n",
      " 'Lynn' 'Bensalem' 'Bristol' 'Williamsport' 'Longview' 'Norwalk' 'Bayonne'\n",
      " 'Tulare' 'National City' 'Dayton' 'Tracy' 'Summerville' 'Merced'\n",
      " 'Brockton' 'Vallejo' 'West Haven' 'Pasadena' 'South Gate' 'Warren'\n",
      " 'Clarksville' 'Muskegon' 'Brandon' 'Rancho Cucamonga' 'Santa Maria'\n",
      " 'Doylestown' 'Colton' 'Indio' 'Plainfield' 'Bellingham' 'Spring'\n",
      " 'Livermore' 'Santa Fe' 'Palo Alto' 'Henrico' 'Des Plaines' 'Birmingham'\n",
      " 'Broomfield' 'Guaynabo' 'Cayey' 'Citrus Heights' 'Spokane' 'Dubuque'\n",
      " 'Madera' 'Everett' 'Brentwood' 'Morganton' 'Vacaville' 'Malden'\n",
      " 'Gwynn Oak' 'Toa Baja' 'Taunton' 'Freehold' 'Sumner' 'Wilmington' 'CA']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Country\n",
      "['Puerto Rico' 'EE. UU.']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Email\n",
      "['XXXXXXXXX']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Fname\n",
      "['Cally' 'Irene' 'Gillian' 'Tana' 'Orli' 'Kimberly' 'Constance' 'Erica'\n",
      " 'Nichole' 'Oprah' 'Germane' 'Freya' 'Cassandra' 'Natalie' 'Kimberley'\n",
      " 'Sade' 'Brynne' 'Ciara' 'Bo' 'Kim' 'Kellie' 'Alma' 'Yeo' 'Lucy' 'Simone'\n",
      " 'Roary' 'Quail' 'Hannah' 'Evelyn' 'Jael' 'Teagan' 'Joan' 'Basia'\n",
      " 'Dominique' 'Ainsley' 'Aurora' 'Libby' 'Amy' 'Ivory' 'Karina' 'Deanna'\n",
      " 'Dacey' 'Adele' 'Sybill' 'Kay' 'Stacy' 'Marah' 'Taylor' 'Mary'\n",
      " 'Elizabeth' 'Katherine' 'Dennis' 'Frances' 'Alice' 'Kyle' 'Brenda'\n",
      " 'Frank' 'Johnny' 'Austin' 'Michael' 'Vincent' 'Gregory' 'Lisa' 'Megan'\n",
      " 'Grace' 'Douglas' 'Wayne' 'Ronald' 'Tammy' 'Christina' 'Alexander'\n",
      " 'Sandra' 'Ruth' 'Lauren' 'Melissa' 'Maria' 'Eugene' 'James' 'John' 'Roy'\n",
      " 'Janet' 'Nicole' 'Sharon' 'Matthew' 'Anthony' 'Jerry' 'Laura' 'Jack'\n",
      " 'Kirestin' 'Nora' 'Jennifer' 'Madonna' 'Noelani' 'Sydnee' 'Willa' 'Ila'\n",
      " 'Belle' 'Alyssa' 'Lani' 'Demetria' 'Gay' 'Audrey' 'Riley' 'Imani'\n",
      " 'Mechelle' 'Whitney' 'Roanna' 'Vivian' 'Xantha' 'Zia' 'Mara' 'Sloane'\n",
      " 'Winifred' 'Giselle' 'Nancy' 'David' 'Patricia' 'Julie' 'Tyler' 'Karen'\n",
      " 'Shawn' 'Willie' 'Bruce' 'Mark' 'Paul' 'Walter' 'Joyce' 'Daniel' 'Pamela'\n",
      " 'Betty' 'Christian' 'Tiffany' 'Stephen' 'George' 'Samuel' 'Larry' 'Peter'\n",
      " 'Diana' 'Theresa' 'Cynthia' 'Sean' 'Jonathan' 'Christopher' 'Donald'\n",
      " 'Rana' 'William' 'Rachel' 'Dorothy' 'Robert' 'Andrea' 'Martha' 'Carolyn'\n",
      " 'Lynn' 'Julia' 'Patience' 'Cheryl' 'Timothy' 'Aimee' 'Virginia' 'Jessica'\n",
      " 'Margaret' 'Richard' 'Deborah' 'Emma' 'Adam' 'Sarah' 'Edward' 'Harry'\n",
      " 'Charles' 'Debra' 'Doris' 'Benjamin' 'Stephanie' 'Nicholas' 'Joseph'\n",
      " 'Diane' 'Andrew' 'Roger' 'Danielle' 'Patrick' 'Brandon' 'Jacqueline'\n",
      " 'Kathy' 'Rebecca' 'Jeremy' 'Amanda' 'Felicia' 'Wanda' 'Hayfa' 'Lavinia'\n",
      " 'Yetta' 'Cleo' 'Wynter' 'Hyacinth' 'Xerxes' 'Noel' 'Maryam' 'Naida'\n",
      " 'Lana' 'Rylee' 'Dakota' 'Fallon' 'Kylynn' 'Inez' 'Miriam' 'Chloe'\n",
      " 'Beverly' 'Madeline' 'Yoshi' 'Hadley' 'Renee' 'Iola' 'Tamekah' 'Cherokee'\n",
      " 'Odessa' 'Rose' 'Linda' 'May' 'Lilah' 'Emi' 'Myra' 'Medge' 'Beatrice'\n",
      " 'Ria' 'Charity' 'Nell' 'Tamara' 'Phyllis' 'Nelle' 'Blossom' 'Breanna'\n",
      " 'Katelyn' 'Karly' 'Olga' 'Brenna' 'Tatyana' 'Ifeoma' 'Kitra' 'Summer'\n",
      " 'Sopoline' 'Eliana' 'Denise' 'Hermione' 'Lara' 'Hedy' 'Violet' 'Clare'\n",
      " 'Rhonda' 'Morgan' 'Illiana' 'Amity' 'Jayme' 'Geraldine' 'Anika' 'Dana'\n",
      " 'Sybil' 'Genevieve' 'Tara' 'Ella' 'Heather' 'Brittany' 'Lori' 'Billy'\n",
      " 'Terry' 'Lawrence' 'Aaron' 'Madison' 'Jane' 'Jordan' 'Kevin' 'Octavia'\n",
      " 'Emily' 'Kelsie' 'Marie' 'Judy' 'Kenneth' 'Jaime' 'Justin' 'Harold'\n",
      " 'Ryan' 'Kelly' 'Carl' 'Teresa' 'Nathan' 'Jason' 'Steven' 'Judith' 'Sara'\n",
      " 'Marilyn' 'Kathryn' 'Ralph' 'Amber' 'Donna' 'Scott' 'Thomas' 'Catherine'\n",
      " 'Barbara' 'Joshua' 'Susan' 'Zachary' 'Janice' 'Crystal' 'Michelle'\n",
      " 'Henry' 'Anna' 'Ann' 'Jesse' 'Raymond' 'Audra' 'Cora' 'Hillary' 'Bryan'\n",
      " 'Gloria' 'Louis' 'Shirley' 'Martina' 'Delilah' 'Jacob' 'Brian' 'Victoria'\n",
      " 'Jeffrey' 'Helen' 'Gary' 'Ethan' 'Philip' 'Christine' 'Ashley' 'Jean'\n",
      " 'Eric' 'Hope' 'Unity' 'Blythe' 'Cara' 'Jorden' 'Heidi' 'Lenore' 'Aiko'\n",
      " 'Kylie' 'Moana' 'Imelda' 'Mariam' 'Rae' 'Jamalia' 'Reagan' 'Amena'\n",
      " 'Mercedes' 'Brianna' 'Xyla' 'Daria' 'Yvette' 'Ina' 'Fay' 'Destiny'\n",
      " 'Petra' 'Farrah' 'Clio' 'Leigh' 'Tallulah' 'Pearl' 'Maya' 'Kiayada'\n",
      " 'Shannon' 'Nayda' 'Charissa' 'Fatima' 'Shelley' 'Yoko' 'Olympia' 'Raya'\n",
      " 'Quyn' 'Hilary' 'Karyn' 'Germaine' 'Maile' 'Vivien' 'Raven' 'Charlotte'\n",
      " 'Zelda' 'Isabella' 'Carla' 'Aspen' 'Janna' 'Xena' 'Ocean' 'Hedwig' 'Alea'\n",
      " 'Aileen' 'Cameron' 'Cailin' 'Jeanette' 'Suki' 'Maxine' 'Sophia' 'Skyler'\n",
      " 'Yvonne' 'Penelope' 'Tatum' 'Sylvia' 'Amela' 'Keiko' 'Shana' 'Whoopi'\n",
      " 'Mira' 'Jaden' 'Pascale' 'Maris' 'Cassady' 'Quynn' 'Sage' 'Calista'\n",
      " 'Sacha' 'Daryl' 'Aphrodite' 'Olivia' 'Mallory' 'Veronica' 'Yuri' 'Joelle'\n",
      " 'Hiroko' 'Kameko' 'Hollee' 'Eden' 'Lydia' 'Azalia' 'Alika' 'Dahlia'\n",
      " 'Jenette' 'Paloma' 'Xandra' 'Mariko' 'Darryl' 'Fiona' 'Zephr' 'Uta'\n",
      " 'Wendy' 'Sierra' 'Kelsey' 'Alexis' 'September' 'McKenzie' 'Eleanor'\n",
      " 'Chastity' 'Cheyenne' 'Hanna' 'Mona' 'Mari' 'Robin' 'Samantha' 'Gerald'\n",
      " 'Kyla' 'Xaviera' 'Macy' 'Briar' 'Athena' 'Desirae' 'Maia' 'Carol'\n",
      " 'Angela' 'Carissa' 'Zena' 'Phillip' 'Bobby' 'Blair' 'Melodie' 'Keith'\n",
      " 'India' 'Nadine' 'Miranda' 'Piper' 'Charde' 'Russell' 'Isabelle'\n",
      " 'Mildred' 'Dylan' 'Albert' 'Arthur' 'Kathleen' 'Juan' 'Cathleen' 'Jose'\n",
      " 'Holly' 'Zenaida' 'Ramona' 'Zorita' 'Jocelyn' 'Alan' 'Mia' 'Justina'\n",
      " 'Rosalyn' 'Rinah' 'Remedios' 'Shay' 'Aubrey' 'Neve' 'Lacy' 'Leilani'\n",
      " 'Shelby' 'Stella' 'Camille' 'Carly' 'Teegan' 'Lareina' 'Kirby' 'Kevyn'\n",
      " 'Gisela' 'Randy' 'Willow' 'Jolie' 'Jade' 'Gwendolyn' 'Lee' 'Celeste'\n",
      " 'Noelle' 'Brynn' 'Desiree' 'Christen' 'Gemma' 'Kendall' 'Bethany'\n",
      " 'Callie' 'Ayanna' 'Zoe' 'Brooke' 'Bree' 'Adrienne' 'Nomlanga' 'Sig'\n",
      " 'Stela' 'Ulla' 'Lesley' 'TaShya' 'Illana' 'Courtney' 'Larissa' 'Juliet'\n",
      " 'Melyssa' 'Latifah' 'Angelica' 'Eve' 'Melanie' 'Montana' 'Nevada'\n",
      " 'Abigail' 'Ivy' 'Lacota' 'Stacey' 'Meghan' 'Joe' 'Howard' 'Winter'\n",
      " 'Jolene' 'Ursa' 'Camilla' 'Iris' 'Bell' 'Chanda' 'Rhiannon' 'Candice'\n",
      " 'Regan' 'Avye' 'Whilemina' 'Lisandra' 'Guinevere' 'Ivana' 'Velma' 'Rhea'\n",
      " 'Dora' 'Lael' 'Echo' 'Kyra' 'Rhoda' 'Jana' 'Minerva' 'Jenna' 'Inga'\n",
      " 'Quemby' 'Abra' 'Quincy' 'Maisie' 'Leila' 'Rossy' 'Charly' 'Iona'\n",
      " 'Ignacia' 'Marcia' 'Florence' 'Ariel' 'Yen' 'Rowan' 'Halee' 'Nita'\n",
      " 'Signe' 'Kessie' 'Maggy' 'Aretha' 'Dawn' 'Elaine' 'Meredith' 'Josephine'\n",
      " 'Maite' 'Anjolie' 'Melinda' 'Kirsten' 'Chelsea' 'Lois' 'Britanney'\n",
      " 'Jillian' 'Shoshana' 'Bertha' 'Katell' 'Isadora' 'Serina' 'Scarlett'\n",
      " 'Selma' 'Eugenia' 'Alexa' 'Quintessa' 'Shellie' 'Maggie' 'Caryn'\n",
      " 'Evangeline' 'Darrel' 'Idola' 'Vielka' 'Clementine' 'Jescie' 'Sydney'\n",
      " 'Quin' 'Fredericka' 'Chantale' 'Buffy' 'Yael' 'Kai' 'Galena' 'Haviva'\n",
      " 'Halla' 'April' 'Alfreda' 'Paula' 'Gail' 'Zenia' 'Jessamine' 'Mikayla'\n",
      " 'Irma' 'Ingrid' 'Chava' 'Laurel' 'Chiquita' 'Marny' 'Faith' 'Amelia'\n",
      " 'Tashya' 'Tatiana' 'Wynne' 'Karleigh' 'Phoebe' 'Rebekah' 'Regina'\n",
      " 'Robina' 'Yolanda' 'Glenna' 'Nyssa' 'Kaitlin' 'Vera' 'Sheila' 'Bryar'\n",
      " 'Ima' 'Haley' 'Kamy' 'Emerald' 'Portia' 'Adara' 'Nerea' 'Harriet' 'Shea'\n",
      " 'Jemima' 'Lacey' 'Ora' 'Odette' 'Kaye' 'Joanha' 'Leandra' 'Indira'\n",
      " 'Lunea' 'Keelie' 'Astra' 'Claire' 'Jaquelyn' 'Sonia' 'Kalia' 'Liberty'\n",
      " 'Anastasia' 'Kiara' 'Ursula' 'Karla' 'Venus' 'Shaine' 'Kerry' 'Susy'\n",
      " 'Rina' 'Ginger' 'Mike' 'Claudia' 'Pandora' 'Veda' 'Jena' 'Lillian'\n",
      " 'Deirdre' 'Indigo' 'Lila' 'Hanae' 'Adena' 'Sonya' 'Anne' 'Oliver'\n",
      " 'Imogene' 'Alexandra' 'Ori' 'Cecilia' 'Lillith' 'Rama' 'Francesca'\n",
      " 'Hadassah' 'Nola' 'Idona' 'Allegra' 'Shafira' 'Shaeleigh' 'Deme' 'Blaine'\n",
      " 'Gretchen' 'Madeson' 'Molly' 'Martena' 'Hedda' 'Heidy' 'Filipe' 'Deysi'\n",
      " 'Brie' 'Alisa' 'Contreras' 'Quon' 'Tasha' 'Lynna' 'Lysandra' 'Priscilla'\n",
      " 'Britanni' 'Lina' 'Autumn' 'Risa' 'Cami' 'Ana' 'Leslie' 'Nadin' 'Naomi'\n",
      " 'Ray' 'Larisa' 'Rebeca' 'Jordana' 'Blanca' 'Sybi' 'Mollie' 'BlueOneal'\n",
      " 'Flora' 'Ariana' 'Anita' 'York' 'Mally' 'Rosa' 'Caro' 'Farley' 'Jenny'\n",
      " 'Danna' 'Fiora' 'Jay' 'Asley' 'Clau' 'Le' 'Huff' 'Beatty' 'Paola' 'Brees']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Password\n",
      "['XXXXXXXXX']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Segment\n",
      "['Consumer' 'Home Office' 'Corporate']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer State\n",
      "['PR' 'CA' 'NY' 'FL' 'MA' 'IL' 'MT' 'PA' 'MI' 'TX' 'DE' 'GA' 'MD' 'OH'\n",
      " 'HI' 'NJ' 'WI' 'AZ' 'CO' 'MN' 'NC' 'NM' 'OR' 'SC' 'VA' 'UT' 'WA' 'KY'\n",
      " 'WV' 'RI' 'CT' 'LA' 'TN' 'DC' 'ND' 'MO' 'IN' 'ID' 'NV' 'KS' 'AR' 'OK'\n",
      " 'AL' 'IA' '95758' '91732']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Customer Street\n",
      "['5365 Noble Nectar Island' '2679 Rustic Loop' '8510 Round Bear Gate' ...\n",
      " '245 Lost Way ' '2455 Merry Hollow ' '8621 Broad Forest ']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Department Name\n",
      "['Fitness' 'Apparel' 'Golf' 'Footwear' 'Outdoors' 'Fan Shop' 'Technology'\n",
      " 'Book Shop' 'Discs Shop' 'Pet Shop' 'Health and Beauty ']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Market\n",
      "['Pacific Asia' 'USCA' 'Africa' 'Europe' 'LATAM']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Order City\n",
      "['Bekasi' 'Bikaner' 'Townsville' ... 'Tongling' 'Liuyang' 'Nashua']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Order Country\n",
      "['Indonesia' 'India' 'Australia' 'China' 'Japón' 'Corea del Sur'\n",
      " 'Singapur' 'Turquía' 'Mongolia' 'Estados Unidos' 'Nigeria'\n",
      " 'República Democrática del Congo' 'Senegal' 'Marruecos' 'Alemania'\n",
      " 'Francia' 'Países Bajos' 'Reino Unido' 'Guatemala' 'El Salvador' 'Panamá'\n",
      " 'República Dominicana' 'Venezuela' 'Colombia' 'Honduras' 'Brasil'\n",
      " 'México' 'Uruguay' 'Argentina' 'Cuba' 'Perú' 'Nicaragua' 'Ecuador'\n",
      " 'Angola' 'Sudán' 'Somalia' 'Costa de Marfil' 'Egipto' 'Italia' 'España'\n",
      " 'Suecia' 'Austria' 'Canada' 'Madagascar' 'Argelia' 'Liberia' 'Zambia'\n",
      " 'Níger' 'SudAfrica' 'Mozambique' 'Tanzania' 'Ruanda' 'Israel'\n",
      " 'Nueva Zelanda' 'Bangladés' 'Tailandia' 'Irak' 'Arabia Saudí' 'Filipinas'\n",
      " 'Kazajistán' 'Irán' 'Myanmar (Birmania)' 'Uzbekistán' 'Benín' 'Camerún'\n",
      " 'Kenia' 'Togo' 'Ucrania' 'Polonia' 'Portugal' 'Rumania'\n",
      " 'Trinidad y Tobago' 'Afganistán' 'Pakistán' 'Vietnam' 'Malasia'\n",
      " 'Finlandia' 'Rusia' 'Irlanda' 'Noruega' 'Eslovaquia' 'Bélgica' 'Bolivia'\n",
      " 'Chile' 'Jamaica' 'Yemen' 'Ghana' 'Guinea' 'Etiopía' 'Bulgaria'\n",
      " 'Kirguistán' 'Georgia' 'Nepal' 'Emiratos Árabes Unidos' 'Camboya'\n",
      " 'Uganda' 'Lesoto' 'Lituania' 'Suiza' 'Hungría' 'Dinamarca' 'Haití'\n",
      " 'Bielorrusia' 'Croacia' 'Laos' 'Baréin' 'Macedonia' 'República Checa'\n",
      " 'Sri Lanka' 'Zimbabue' 'Eritrea' 'Burkina Faso' 'Costa Rica' 'Libia'\n",
      " 'Barbados' 'Tayikistán' 'Siria' 'Guadalupe' 'Papúa Nueva Guinea'\n",
      " 'Azerbaiyán' 'Turkmenistán' 'Paraguay' 'Jordania' 'Hong Kong' 'Martinica'\n",
      " 'Moldavia' 'Qatar' 'Mali' 'Albania' 'República del Congo'\n",
      " 'Bosnia y Herzegovina' 'Omán' 'Túnez' 'Sierra Leona' 'Yibuti' 'Burundi'\n",
      " 'Montenegro' 'Gabón' 'Sudán del Sur' 'Luxemburgo' 'Namibia' 'Mauritania'\n",
      " 'Grecia' 'Suazilandia' 'Guyana' 'Guayana Francesa'\n",
      " 'República Centroafricana' 'Taiwán' 'Estonia' 'Líbano' 'Chipre'\n",
      " 'Guinea-Bissau' 'Surinam' 'Belice' 'Eslovenia' 'República de Gambia'\n",
      " 'Botsuana' 'Armenia' 'Guinea Ecuatorial' 'Kuwait' 'Bután' 'Chad' 'Serbia'\n",
      " 'Sáhara Occidental']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "order date (DateOrders)\n",
      "['1/31/2018 22:56' '1/13/2018 12:27' '1/13/2018 12:06' ...\n",
      " '1/21/2016 2:47' '1/20/2016 7:10' '1/17/2016 5:56']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Order Region\n",
      "['Southeast Asia' 'South Asia' 'Oceania' 'Eastern Asia' 'West Asia'\n",
      " 'West of USA ' 'US Center ' 'West Africa' 'Central Africa' 'North Africa'\n",
      " 'Western Europe' 'Northern Europe' 'Central America' 'Caribbean'\n",
      " 'South America' 'East Africa' 'Southern Europe' 'East of USA' 'Canada'\n",
      " 'Southern Africa' 'Central Asia' 'Eastern Europe' 'South of  USA ']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Order State\n",
      "['Java Occidental' 'Rajastán' 'Queensland' ... 'Bistrita-Nasaud' 'Tottori'\n",
      " 'Khorezm']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Order Status\n",
      "['COMPLETE' 'PENDING' 'CLOSED' 'PENDING_PAYMENT' 'CANCELED' 'PROCESSING'\n",
      " 'SUSPECTED_FRAUD' 'ON_HOLD' 'PAYMENT_REVIEW']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Product Image\n",
      "['http://images.acmesports.sports/Smart+watch '\n",
      " 'http://images.acmesports.sports/Perfect+Fitness+Perfect+Rip+Deck'\n",
      " 'http://images.acmesports.sports/Under+Armour+Girls%27+Toddler+Spine+Surge+Running+Shoe'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Dri-FIT+Victory+Golf+Polo'\n",
      " 'http://images.acmesports.sports/Under+Armour+Men%27s+Compression+EV+SL+Slide'\n",
      " 'http://images.acmesports.sports/Under+Armour+Women%27s+Micro+G+Skulpt+Running+Shoe'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Free+5.0%2B+Running+Shoe'\n",
      " 'http://images.acmesports.sports/Glove+It+Women%27s+Mod+Oval+3-Zip+Carry+All+Golf+Bag'\n",
      " 'http://images.acmesports.sports/Bridgestone+e6+Straight+Distance+NFL+San+Diego+Chargers...'\n",
      " 'http://images.acmesports.sports/Columbia+Men%27s+PFG+Anchor+Tough+T-Shirt'\n",
      " 'http://images.acmesports.sports/Titleist+Pro+V1x+Golf+Balls'\n",
      " 'http://images.acmesports.sports/Bridgestone+e6+Straight+Distance+NFL+Tennessee+Titans+Golf...'\n",
      " 'http://images.acmesports.sports/Polar+FT4+Heart+Rate+Monitor'\n",
      " 'http://images.acmesports.sports/ENO+Atlas+Hammock+Straps'\n",
      " 'http://images.acmesports.sports/adidas+Men%27s+F10+Messi+TRX+FG+Soccer+Cleat'\n",
      " 'http://images.acmesports.sports/Brooks+Women%27s+Ghost+6+Running+Shoe'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+CJ+Elite+2+TD+Football+Cleat'\n",
      " 'http://images.acmesports.sports/Diamondback+Women%27s+Serene+Classic+Comfort+Bike+2014'\n",
      " 'http://images.acmesports.sports/Industrial+consumer+electronics'\n",
      " 'http://images.acmesports.sports/Web+Camera'\n",
      " 'http://images.acmesports.sports/Dell+Laptop'\n",
      " 'http://images.acmesports.sports/SOLE+E25+Elliptical'\n",
      " 'http://images.acmesports.sports/Elevation+Training+Mask+2.0'\n",
      " 'http://images.acmesports.sports/adidas+Men%27s+Germany+Black+Crest+Away+Tee'\n",
      " 'http://images.acmesports.sports/Team+Golf+Pittsburgh+Steelers+Putter+Grip'\n",
      " 'http://images.acmesports.sports/Glove+It+Urban+Brick+Golf+Towel'\n",
      " 'http://images.acmesports.sports/Team+Golf+Texas+Longhorns+Putter+Grip'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Deutschland+Weltmeister+Winners+Black+T-Shirt'\n",
      " 'http://images.acmesports.sports/Team+Golf+St.+Louis+Cardinals+Putter+Grip'\n",
      " 'http://images.acmesports.sports/Summer+dresses'\n",
      " 'http://images.acmesports.sports/Porcelain+crafts'\n",
      " 'http://images.acmesports.sports/Men+gala+suit'\n",
      " 'http://images.acmesports.sports/Team+Golf+Tennessee+Volunteers+Putter+Grip'\n",
      " 'http://images.acmesports.sports/Team+Golf+San+Francisco+Giants+Putter+Grip'\n",
      " 'http://images.acmesports.sports/Glove+It+Imperial+Golf+Towel'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Comfort+2+Slide'\n",
      " 'http://images.acmesports.sports/Under+Armour+Hustle+Storm+Medium+Duffle+Bag'\n",
      " 'http://images.acmesports.sports/Under+Armour+Kids%27+Mercenary+Slide'\n",
      " 'http://images.acmesports.sports/Under+Armour+Women%27s+Ignite+PIP+VI+Slide'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Free+TR+5.0+TB+Training+Shoe'\n",
      " 'http://images.acmesports.sports/adidas+Youth+Germany+Black%2FRed+Away+Match+Soccer+Jersey'\n",
      " 'http://images.acmesports.sports/TYR+Boys%27+Team+Digi+Jammer'\n",
      " 'http://images.acmesports.sports/Glove+It+Women%27s+Imperial+Golf+Glove'\n",
      " 'http://images.acmesports.sports/Titleist+Pro+V1x+High+Numbers+Golf+Balls'\n",
      " 'http://images.acmesports.sports/Bridgestone+e6+Straight+Distance+NFL+Carolina+Panthers+Golf...'\n",
      " 'http://images.acmesports.sports/Under+Armour+Women%27s+Ignite+Slide'\n",
      " 'http://images.acmesports.sports/Titleist+Pro+V1x+High+Numbers+Personalized+Golf+Balls'\n",
      " 'http://images.acmesports.sports/GoPro+HERO3%2B+Black+Edition+Camera'\n",
      " 'http://images.acmesports.sports/Total+Gym+1400'\n",
      " 'http://images.acmesports.sports/Children+heaters'\n",
      " 'http://images.acmesports.sports/Team+Golf+New+England+Patriots+Putter+Grip'\n",
      " 'http://images.acmesports.sports/adidas+Kids%27+F5+Messi+FG+Soccer+Cleat'\n",
      " 'http://images.acmesports.sports/Nike+Women%27s+Tempo+Shorts'\n",
      " 'http://images.acmesports.sports/Glove+It+Women%27s+Mod+Oval+Golf+Glove'\n",
      " 'http://images.acmesports.sports/Titleist+Pro+V1+High+Numbers+Personalized+Golf+Balls'\n",
      " 'http://images.acmesports.sports/Under+Armour+Men%27s+Tech+II+T-Shirt'\n",
      " 'http://images.acmesports.sports/Baby+sweater'\n",
      " 'http://images.acmesports.sports/Mio+ALPHA+Heart+Rate+Monitor%2FSport+Watch'\n",
      " 'http://images.acmesports.sports/Field+%26+Stream+Sportsman+16+Gun+Fire+Safe'\n",
      " 'http://images.acmesports.sports/Sports+Books'\n",
      " 'http://images.acmesports.sports/Diamondback+Boys%27+Insight+24+Performance+Hybrid+Bike+2014'\n",
      " 'http://images.acmesports.sports/Polar+Loop+Activity+Tracker'\n",
      " 'http://images.acmesports.sports/Garmin+Forerunner+910XT+GPS+Watch'\n",
      " 'http://images.acmesports.sports/DVDs'\n",
      " 'http://images.acmesports.sports/CDs+of+rock'\n",
      " 'http://images.acmesports.sports/Nike+Kids%27+Grade+School+KD+VI+Basketball+Shoe'\n",
      " 'http://images.acmesports.sports/Nike+Women%27s+Free+5.0+TR+FIT+PRT+4+Training+Shoe'\n",
      " 'http://images.acmesports.sports/Hirzl+Women%27s+Soffft+Flex+Golf+Glove'\n",
      " 'http://images.acmesports.sports/The+North+Face+Women%27s+Recon+Backpack'\n",
      " 'http://images.acmesports.sports/Lawn+mower'\n",
      " 'http://images.acmesports.sports/Nike+Dri-FIT+Crew+Sock+6+Pack'\n",
      " 'http://images.acmesports.sports/Nike+Women%27s+Legend+V-Neck+T-Shirt'\n",
      " 'http://images.acmesports.sports/Garmin+Approach+S4+Golf+GPS+Watch'\n",
      " 'http://images.acmesports.sports/insta-bed+Neverflat+Air+Mattress'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Kobe+IX+Elite+Low+Basketball+Shoe'\n",
      " 'http://images.acmesports.sports/Adult+dog+supplies'\n",
      " 'http://images.acmesports.sports/First+aid+kit'\n",
      " 'http://images.acmesports.sports/Garmin+Approach+S3+Golf+GPS+Watch'\n",
      " 'http://images.acmesports.sports/Rock+music'\n",
      " 'http://images.acmesports.sports/Fighting+video+games'\n",
      " 'http://images.acmesports.sports/Fitbit+The+One+Wireless+Activity+%26+Sleep+Tracker'\n",
      " 'http://images.acmesports.sports/Stiga+Master+Series+ST3100+Competition+Indoor+Table+Tennis...'\n",
      " 'http://images.acmesports.sports/Diamondback+Girls%27+Clarity+24+Hybrid+Bike+2014'\n",
      " 'http://images.acmesports.sports/adidas+Brazuca+2014+Official+Match+Ball'\n",
      " 'http://images.acmesports.sports/GolfBuddy+VT3+GPS+Watch'\n",
      " 'http://images.acmesports.sports/Bushnell+Pro+X7+Jolt+Slope+Rangefinder'\n",
      " 'http://images.acmesports.sports/Yakima+DoubleDown+Ace+Hitch+Mount+4-Bike+Rack'\n",
      " 'http://images.acmesports.sports/Nike+Men%27s+Fingertrap+Max+Training+Shoe'\n",
      " 'http://images.acmesports.sports/Bowflex+SelectTech+1090+Dumbbells'\n",
      " 'http://images.acmesports.sports/SOLE+E35+Elliptical'\n",
      " 'http://images.acmesports.sports/Hirzl+Women%27s+Hybrid+Golf+Glove'\n",
      " 'http://images.acmesports.sports/Hirzl+Men%27s+Hybrid+Golf+Glove'\n",
      " 'http://images.acmesports.sports/TaylorMade+2014+Purelite+Stand+Bag'\n",
      " 'http://images.acmesports.sports/Bag+Boy+Beverage+Holder'\n",
      " 'http://images.acmesports.sports/Bag+Boy+M330+Push+Cart'\n",
      " 'http://images.acmesports.sports/Clicgear+8.0+Shoe+Brush'\n",
      " 'http://images.acmesports.sports/Titleist+Small+Wheeled+Travel+Cover'\n",
      " 'http://images.acmesports.sports/Clicgear+Rovic+Cooler+Bag'\n",
      " 'http://images.acmesports.sports/Titleist+Club+Glove+Travel+Cover'\n",
      " 'http://images.acmesports.sports/Ogio+Race+Golf+Shoes'\n",
      " 'http://images.acmesports.sports/LIJA+Women%27s+Argyle+Golf+Polo'\n",
      " 'http://images.acmesports.sports/LIJA+Women%27s+Eyelet+Sleeveless+Golf+Polo'\n",
      " 'http://images.acmesports.sports/LIJA+Women%27s+Button+Golf+Dress'\n",
      " 'http://images.acmesports.sports/LIJA+Women%27s+Mid-Length+Panel+Golf+Shorts'\n",
      " 'http://images.acmesports.sports/TaylorMade+Women%27s+RBZ+SL+Rescue'\n",
      " 'http://images.acmesports.sports/Cleveland+Golf+Women%27s+588+RTX+CB+Satin+Chrome+Wedge'\n",
      " 'http://images.acmesports.sports/Top+Flite+Women%27s+2014+XL+Hybrid'\n",
      " 'http://images.acmesports.sports/MDGolf+Pittsburgh+Penguins+Putter'\n",
      " 'http://images.acmesports.sports/TaylorMade+White+Smoke+IN-12+Putter'\n",
      " 'http://images.acmesports.sports/Cleveland+Golf+Collegiate+My+Custom+Wedge+588+RTX+Forged...'\n",
      " 'http://images.acmesports.sports/Merrell+Men%27s+All+Out+Flash+Trail+Running+Shoe'\n",
      " 'http://images.acmesports.sports/Merrell+Women%27s+Grassbow+Sport+Waterproof+Hiking+Shoe'\n",
      " 'http://images.acmesports.sports/Merrell+Women%27s+Siren+Mid+Waterproof+Hiking+Boot'\n",
      " 'http://images.acmesports.sports/Merrell+Women%27s+Grassbow+Sport+Hiking+Shoe'\n",
      " 'http://images.acmesports.sports/Toys '\n",
      " 'http://images.acmesports.sports/Pelican+Sunstream+100+Kayak'\n",
      " 'http://images.acmesports.sports/Pelican+Maverick+100X+Kayak'\n",
      " 'http://images.acmesports.sports/O%27Brien+Men%27s+Neoprene+Life+Vest']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Product Name\n",
      "['Smart watch ' 'Perfect Fitness Perfect Rip Deck'\n",
      " \"Under Armour Girls' Toddler Spine Surge Runni\"\n",
      " \"Nike Men's Dri-FIT Victory Golf Polo\"\n",
      " \"Under Armour Men's Compression EV SL Slide\"\n",
      " \"Under Armour Women's Micro G Skulpt Running S\"\n",
      " \"Nike Men's Free 5.0+ Running Shoe\"\n",
      " \"Glove It Women's Mod Oval 3-Zip Carry All Gol\"\n",
      " 'Bridgestone e6 Straight Distance NFL San Dieg'\n",
      " \"Columbia Men's PFG Anchor Tough T-Shirt\" 'Titleist Pro V1x Golf Balls'\n",
      " 'Bridgestone e6 Straight Distance NFL Tennesse'\n",
      " 'Polar FT4 Heart Rate Monitor' 'ENO Atlas Hammock Straps'\n",
      " \"adidas Men's F10 Messi TRX FG Soccer Cleat\"\n",
      " \"Brooks Women's Ghost 6 Running Shoe\"\n",
      " \"Nike Men's CJ Elite 2 TD Football Cleat\"\n",
      " \"Diamondback Women's Serene Classic Comfort Bi\"\n",
      " 'Industrial consumer electronics' 'Web Camera' 'Dell Laptop'\n",
      " 'SOLE E25 Elliptical' 'Elevation Training Mask 2.0'\n",
      " \"adidas Men's Germany Black Crest Away Tee\"\n",
      " 'Team Golf Pittsburgh Steelers Putter Grip'\n",
      " 'Glove It Urban Brick Golf Towel' 'Team Golf Texas Longhorns Putter Grip'\n",
      " \"Nike Men's Deutschland Weltmeister Winners Bl\"\n",
      " 'Team Golf St. Louis Cardinals Putter Grip' 'Summer dresses'\n",
      " 'Porcelain crafts' \"Men's gala suit\"\n",
      " 'Team Golf Tennessee Volunteers Putter Grip'\n",
      " 'Team Golf San Francisco Giants Putter Grip'\n",
      " 'Glove It Imperial Golf Towel' \"Nike Men's Comfort 2 Slide\"\n",
      " 'Under Armour Hustle Storm Medium Duffle Bag'\n",
      " \"Under Armour Kids' Mercenary Slide\"\n",
      " \"Under Armour Women's Ignite PIP VI Slide\"\n",
      " \"Nike Men's Free TR 5.0 TB Training Shoe\"\n",
      " 'adidas Youth Germany Black/Red Away Match Soc'\n",
      " \"TYR Boys' Team Digi Jammer\" \"Glove It Women's Imperial Golf Glove\"\n",
      " 'Titleist Pro V1x High Numbers Golf Balls'\n",
      " 'Bridgestone e6 Straight Distance NFL Carolina'\n",
      " \"Under Armour Women's Ignite Slide\"\n",
      " 'Titleist Pro V1x High Numbers Personalized Go'\n",
      " 'GoPro HERO3+ Black Edition Camera' 'Total Gym 1400' \"Children's heaters\"\n",
      " 'Team Golf New England Patriots Putter Grip'\n",
      " \"adidas Kids' F5 Messi FG Soccer Cleat\" \"Nike Women's Tempo Shorts\"\n",
      " \"Glove It Women's Mod Oval Golf Glove\"\n",
      " 'Titleist Pro V1 High Numbers Personalized Gol'\n",
      " \"Under Armour Men's Tech II T-Shirt\" 'Baby sweater'\n",
      " 'Mio ALPHA Heart Rate Monitor/Sport Watch'\n",
      " 'Field & Stream Sportsman 16 Gun Fire Safe' 'Sports Books '\n",
      " \"Diamondback Boys' Insight 24 Performance Hybr\"\n",
      " 'Polar Loop Activity Tracker' 'Garmin Forerunner 910XT GPS Watch' 'DVDs '\n",
      " 'CDs of rock' \"Nike Kids' Grade School KD VI Basketball Shoe\"\n",
      " \"Nike Women's Free 5.0 TR FIT PRT 4 Training S\"\n",
      " \"Hirzl Women's Soffft Flex Golf Glove\"\n",
      " \"The North Face Women's Recon Backpack\" 'Lawn mower'\n",
      " 'Nike Dri-FIT Crew Sock 6 Pack' \"Nike Women's Legend V-Neck T-Shirt\"\n",
      " 'Garmin Approach S4 Golf GPS Watch' 'insta-bed Neverflat Air Mattress'\n",
      " \"Nike Men's Kobe IX Elite Low Basketball Shoe\" 'Adult dog supplies'\n",
      " 'First aid kit' 'Garmin Approach S3 Golf GPS Watch' 'Rock music'\n",
      " 'Fighting video games' 'Fitbit The One Wireless Activity & Sleep Trac'\n",
      " 'Stiga Master Series ST3100 Competition Indoor'\n",
      " \"Diamondback Girls' Clarity 24 Hybrid Bike 201\"\n",
      " 'adidas Brazuca 2014 Official Match Ball' 'GolfBuddy VT3 GPS Watch'\n",
      " 'Bushnell Pro X7 Jolt Slope Rangefinder'\n",
      " 'Yakima DoubleDown Ace Hitch Mount 4-Bike Rack'\n",
      " \"Nike Men's Fingertrap Max Training Shoe\"\n",
      " 'Bowflex SelectTech 1090 Dumbbells' 'SOLE E35 Elliptical'\n",
      " \"Hirzl Women's Hybrid Golf Glove\" \"Hirzl Men's Hybrid Golf Glove\"\n",
      " 'TaylorMade 2014 Purelite Stand Bag' 'Bag Boy Beverage Holder'\n",
      " 'Bag Boy M330 Push Cart' 'Clicgear 8.0 Shoe Brush'\n",
      " 'Titleist Small Wheeled Travel Cover' 'Clicgear Rovic Cooler Bag'\n",
      " 'Titleist Club Glove Travel Cover' 'Ogio Race Golf Shoes'\n",
      " \"LIJA Women's Argyle Golf Polo\"\n",
      " \"LIJA Women's Eyelet Sleeveless Golf Polo\"\n",
      " \"LIJA Women's Button Golf Dress\"\n",
      " \"LIJA Women's Mid-Length Panel Golf Shorts\"\n",
      " \"TaylorMade Women's RBZ SL Rescue\"\n",
      " \"Cleveland Golf Women's 588 RTX CB Satin Chrom\"\n",
      " \"Top Flite Women's 2014 XL Hybrid\" 'MDGolf Pittsburgh Penguins Putter'\n",
      " 'TaylorMade White Smoke IN-12 Putter'\n",
      " 'Cleveland Golf Collegiate My Custom Wedge 588'\n",
      " \"Merrell Men's All Out Flash Trail Running Sho\"\n",
      " \"Merrell Women's Grassbow Sport Waterproof Hik\"\n",
      " \"Merrell Women's Siren Mid Waterproof Hiking B\"\n",
      " \"Merrell Women's Grassbow Sport Hiking Shoe\" 'Toys '\n",
      " 'Pelican Sunstream 100 Kayak' 'Pelican Maverick 100X Kayak'\n",
      " \"O'Brien Men's Neoprene Life Vest\"]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "shipping date (DateOrders)\n",
      "['2/3/2018 22:56' '1/18/2018 12:27' '1/17/2018 12:06' ... '1/25/2016 2:47'\n",
      " '1/23/2016 7:10' '1/21/2016 5:56']\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Shipping Mode\n",
      "['Standard Class' 'First Class' 'Second Class' 'Same Day']\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(category_features)):\n",
    "    print(category_features[i])\n",
    "    print(df[category_features[i]].unique())\n",
    "    print(\"-------------------\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f0c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type\n",
      "4\n",
      "Days for shipping (real)\n",
      "7\n",
      "Days for shipment (scheduled)\n",
      "4\n",
      "Benefit per order\n",
      "21998\n",
      "Sales per customer\n",
      "2927\n",
      "Delivery Status\n",
      "4\n",
      "Late_delivery_risk\n",
      "2\n",
      "Category Id\n",
      "51\n",
      "Category Name\n",
      "50\n",
      "Customer City\n",
      "563\n",
      "Customer Country\n",
      "2\n",
      "Customer Email\n",
      "1\n",
      "Customer Fname\n",
      "782\n",
      "Customer Id\n",
      "20652\n",
      "Customer Lname\n",
      "1110\n",
      "Customer Password\n",
      "1\n",
      "Customer Segment\n",
      "3\n",
      "Customer State\n",
      "46\n",
      "Customer Street\n",
      "7458\n",
      "Customer Zipcode\n",
      "996\n",
      "Department Id\n",
      "11\n",
      "Department Name\n",
      "11\n",
      "Latitude\n",
      "11250\n",
      "Longitude\n",
      "4487\n",
      "Market\n",
      "5\n",
      "Order City\n",
      "3597\n",
      "Order Country\n",
      "164\n",
      "Order Customer Id\n",
      "20652\n",
      "order date (DateOrders)\n",
      "65752\n",
      "Order Id\n",
      "65752\n",
      "Order Item Cardprod Id\n",
      "118\n",
      "Order Item Discount\n",
      "1017\n",
      "Order Item Discount Rate\n",
      "18\n",
      "Order Item Id\n",
      "180519\n",
      "Order Item Product Price\n",
      "75\n",
      "Order Item Profit Ratio\n",
      "162\n",
      "Order Item Quantity\n",
      "5\n",
      "Sales\n",
      "193\n",
      "Order Item Total\n",
      "2927\n",
      "Order Profit Per Order\n",
      "21998\n",
      "Order Region\n",
      "23\n",
      "Order State\n",
      "1089\n",
      "Order Status\n",
      "9\n",
      "Order Zipcode\n",
      "610\n",
      "Product Card Id\n",
      "118\n",
      "Product Category Id\n",
      "51\n",
      "Product Description\n",
      "1\n",
      "Product Image\n",
      "118\n",
      "Product Name\n",
      "118\n",
      "Product Price\n",
      "75\n",
      "Product Status\n",
      "1\n",
      "shipping date (DateOrders)\n",
      "63701\n",
      "Shipping Mode\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)\n",
    "    print(len(df[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bf42d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['order_year'] = pd.to_datetime(df['order date (DateOrders)']).dt.year\n",
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['order_month'] = pd.to_datetime(df['order date (DateOrders)']).dt.month\n",
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['order_day_of_week'] = pd.to_datetime(df['order date (DateOrders)']).dt.weekday\n",
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ship_year'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.year\n",
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ship_month'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.month\n",
      "C:\\Users\\DPhuc\\AppData\\Local\\Temp\\ipykernel_1776\\697431614.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ship_day_of_week'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.weekday\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Days for shipment (scheduled)</th>\n",
       "      <th>Benefit per order</th>\n",
       "      <th>Sales per customer</th>\n",
       "      <th>Delivery Status</th>\n",
       "      <th>Category Id</th>\n",
       "      <th>Category Name</th>\n",
       "      <th>Customer City</th>\n",
       "      <th>Customer Country</th>\n",
       "      <th>Customer Segment</th>\n",
       "      <th>...</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Shipping Mode</th>\n",
       "      <th>Late_delivery_risk</th>\n",
       "      <th>Days for shipping (real)</th>\n",
       "      <th>order_year</th>\n",
       "      <th>order_month</th>\n",
       "      <th>order_day_of_week</th>\n",
       "      <th>ship_year</th>\n",
       "      <th>ship_month</th>\n",
       "      <th>ship_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>4</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>314.640015</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>...</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>4</td>\n",
       "      <td>-249.089996</td>\n",
       "      <td>311.359985</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>...</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASH</td>\n",
       "      <td>4</td>\n",
       "      <td>-247.779999</td>\n",
       "      <td>309.720001</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>EE. UU.</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>...</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>4</td>\n",
       "      <td>22.860001</td>\n",
       "      <td>304.809998</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>EE. UU.</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>...</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>4</td>\n",
       "      <td>134.210007</td>\n",
       "      <td>298.250000</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>...</td>\n",
       "      <td>327.750000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180514</th>\n",
       "      <td>CASH</td>\n",
       "      <td>4</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>EE. UU.</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>...</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180515</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>2</td>\n",
       "      <td>-613.770019</td>\n",
       "      <td>395.980011</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Bakersfield</td>\n",
       "      <td>EE. UU.</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>...</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180516</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>4</td>\n",
       "      <td>141.110001</td>\n",
       "      <td>391.980011</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>EE. UU.</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>...</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180517</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>4</td>\n",
       "      <td>186.229996</td>\n",
       "      <td>387.980011</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>...</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180518</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>4</td>\n",
       "      <td>168.949997</td>\n",
       "      <td>383.980011</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>45</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>...</td>\n",
       "      <td>399.980011</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180519 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Days for shipment (scheduled)  Benefit per order  \\\n",
       "0          DEBIT                              4          91.250000   \n",
       "1       TRANSFER                              4        -249.089996   \n",
       "2           CASH                              4        -247.779999   \n",
       "3          DEBIT                              4          22.860001   \n",
       "4        PAYMENT                              4         134.210007   \n",
       "...          ...                            ...                ...   \n",
       "180514      CASH                              4          40.000000   \n",
       "180515     DEBIT                              2        -613.770019   \n",
       "180516  TRANSFER                              4         141.110001   \n",
       "180517   PAYMENT                              4         186.229996   \n",
       "180518   PAYMENT                              4         168.949997   \n",
       "\n",
       "        Sales per customer   Delivery Status  Category Id   Category Name  \\\n",
       "0               314.640015  Advance shipping           73  Sporting Goods   \n",
       "1               311.359985     Late delivery           73  Sporting Goods   \n",
       "2               309.720001  Shipping on time           73  Sporting Goods   \n",
       "3               304.809998  Advance shipping           73  Sporting Goods   \n",
       "4               298.250000  Advance shipping           73  Sporting Goods   \n",
       "...                    ...               ...          ...             ...   \n",
       "180514          399.980011  Shipping on time           45         Fishing   \n",
       "180515          395.980011     Late delivery           45         Fishing   \n",
       "180516          391.980011     Late delivery           45         Fishing   \n",
       "180517          387.980011  Advance shipping           45         Fishing   \n",
       "180518          383.980011  Shipping on time           45         Fishing   \n",
       "\n",
       "       Customer City Customer Country Customer Segment  ... Product Price  \\\n",
       "0             Caguas      Puerto Rico         Consumer  ...    327.750000   \n",
       "1             Caguas      Puerto Rico         Consumer  ...    327.750000   \n",
       "2           San Jose          EE. UU.         Consumer  ...    327.750000   \n",
       "3        Los Angeles          EE. UU.      Home Office  ...    327.750000   \n",
       "4             Caguas      Puerto Rico        Corporate  ...    327.750000   \n",
       "...              ...              ...              ...  ...           ...   \n",
       "180514      Brooklyn          EE. UU.      Home Office  ...    399.980011   \n",
       "180515   Bakersfield          EE. UU.        Corporate  ...    399.980011   \n",
       "180516       Bristol          EE. UU.        Corporate  ...    399.980011   \n",
       "180517        Caguas      Puerto Rico         Consumer  ...    399.980011   \n",
       "180518        Caguas      Puerto Rico         Consumer  ...    399.980011   \n",
       "\n",
       "         Shipping Mode  Late_delivery_risk  Days for shipping (real)  \\\n",
       "0       Standard Class                   0                         3   \n",
       "1       Standard Class                   1                         5   \n",
       "2       Standard Class                   0                         4   \n",
       "3       Standard Class                   0                         3   \n",
       "4       Standard Class                   0                         2   \n",
       "...                ...                 ...                       ...   \n",
       "180514  Standard Class                   0                         4   \n",
       "180515    Second Class                   1                         3   \n",
       "180516  Standard Class                   1                         5   \n",
       "180517  Standard Class                   0                         3   \n",
       "180518  Standard Class                   0                         4   \n",
       "\n",
       "        order_year  order_month order_day_of_week ship_year ship_month  \\\n",
       "0             2018            1                 2      2018          2   \n",
       "1             2018            1                 5      2018          1   \n",
       "2             2018            1                 5      2018          1   \n",
       "3             2018            1                 5      2018          1   \n",
       "4             2018            1                 5      2018          1   \n",
       "...            ...          ...               ...       ...        ...   \n",
       "180514        2016            1                 5      2016          1   \n",
       "180515        2016            1                 5      2016          1   \n",
       "180516        2016            1                 4      2016          1   \n",
       "180517        2016            1                 4      2016          1   \n",
       "180518        2016            1                 4      2016          1   \n",
       "\n",
       "        ship_day_of_week  \n",
       "0                      5  \n",
       "1                      3  \n",
       "2                      2  \n",
       "3                      1  \n",
       "4                      0  \n",
       "...                  ...  \n",
       "180514                 2  \n",
       "180515                 1  \n",
       "180516                 2  \n",
       "180517                 0  \n",
       "180518                 1  \n",
       "\n",
       "[180519 rows x 46 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Chọn chỉ các cột cần thiết\n",
    "selected_columns =   ['Type', 'Days for shipment (scheduled)',\n",
    "                     'Benefit per order', 'Sales per customer', 'Delivery Status',\n",
    "                    'Category Id', 'Category Name', 'Customer City',\n",
    "                     'Customer Country', 'Customer Segment',\n",
    "                     'Customer State', 'Customer Street', 'Customer Zipcode',\n",
    "                     'Department Id', 'Latitude', 'Longitude', 'Market',\n",
    "                     'Order City', 'Order Country',\n",
    "                     'order date (DateOrders)', 'Order Item Cardprod Id',\n",
    "                     'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id',\n",
    "                     'Order Item Product Price', 'Order Item Profit Ratio',\n",
    "                     'Order Item Quantity', 'Sales', 'Order Item Total',\n",
    "                     'Order Profit Per Order', 'Order Region', 'Order State', 'Order Status',\n",
    "                     'Order Zipcode', 'Product Card Id', 'Product Category Id', 'Product Name', 'Product Price',\n",
    "                     'shipping date (DateOrders)', 'Shipping Mode', 'Late_delivery_risk', 'Days for shipping (real)']\n",
    "\n",
    "df = df[selected_columns]\n",
    "\n",
    "df['order_year'] = pd.to_datetime(df['order date (DateOrders)']).dt.year\n",
    "df['order_month'] = pd.to_datetime(df['order date (DateOrders)']).dt.month\n",
    "df['order_day_of_week'] = pd.to_datetime(df['order date (DateOrders)']).dt.weekday\n",
    "\n",
    "df['ship_year'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.year\n",
    "df['ship_month'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.month\n",
    "df['ship_day_of_week'] = pd.to_datetime(df['shipping date (DateOrders)']).dt.weekday\n",
    "\n",
    "df = df.drop(['shipping date (DateOrders)','order date (DateOrders)'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc20a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type\n",
      "Type\n",
      "DEBIT       0.383865\n",
      "TRANSFER    0.276331\n",
      "PAYMENT     0.231139\n",
      "CASH        0.108664\n",
      "Name: proportion, dtype: float64\n",
      "Delivery Status\n",
      "Delivery Status\n",
      "Late delivery        0.548291\n",
      "Advance shipping     0.230402\n",
      "Shipping on time     0.178352\n",
      "Shipping canceled    0.042954\n",
      "Name: proportion, dtype: float64\n",
      "Category Id\n",
      "Category Id\n",
      "17    0.136002\n",
      "18    0.123234\n",
      "24    0.116525\n",
      "46    0.106903\n",
      "45    0.095973\n",
      "48    0.086085\n",
      "43    0.076053\n",
      "9     0.069173\n",
      "29    0.060847\n",
      "37    0.011240\n",
      "40    0.009860\n",
      "36    0.008171\n",
      "26    0.006653\n",
      "13    0.006243\n",
      "35    0.005927\n",
      "41    0.005396\n",
      "75    0.004642\n",
      "63    0.003612\n",
      "76    0.003601\n",
      "3     0.003501\n",
      "7     0.003401\n",
      "62    0.003279\n",
      "74    0.002930\n",
      "33    0.002903\n",
      "72    0.002725\n",
      "66    0.002681\n",
      "68    0.002681\n",
      "67    0.002676\n",
      "64    0.002448\n",
      "32    0.002443\n",
      "44    0.002437\n",
      "71    0.002404\n",
      "65    0.002388\n",
      "12    0.002343\n",
      "59    0.002244\n",
      "38    0.002127\n",
      "69    0.002005\n",
      "73    0.001978\n",
      "5     0.001900\n",
      "6     0.001817\n",
      "11    0.001712\n",
      "30    0.001568\n",
      "61    0.001501\n",
      "70    0.001152\n",
      "60    0.001147\n",
      "31    0.001003\n",
      "2     0.000764\n",
      "10    0.000615\n",
      "16    0.000377\n",
      "4     0.000371\n",
      "34    0.000338\n",
      "Name: proportion, dtype: float64\n",
      "Category Name\n",
      "Category Name\n",
      "Cleats                  0.136002\n",
      "Men's Footwear          0.123234\n",
      "Women's Apparel         0.116525\n",
      "Indoor/Outdoor Games    0.106903\n",
      "Fishing                 0.095973\n",
      "Water Sports            0.086085\n",
      "Camping & Hiking        0.076053\n",
      "Cardio Equipment        0.069173\n",
      "Shop By Sport           0.060847\n",
      "Electronics             0.017483\n",
      "Accessories             0.009860\n",
      "Golf Balls              0.008171\n",
      "Girls' Apparel          0.006653\n",
      "Golf Gloves             0.005927\n",
      "Trade-In                0.005396\n",
      "Video Games             0.004642\n",
      "Children's Clothing     0.003612\n",
      "Women's Clothing        0.003601\n",
      "Baseball & Softball     0.003501\n",
      "Hockey                  0.003401\n",
      "Cameras                 0.003279\n",
      "Toys                    0.002930\n",
      "Golf Shoes              0.002903\n",
      "Pet Supplies            0.002725\n",
      "Garden                  0.002681\n",
      "Crafts                  0.002681\n",
      "DVDs                    0.002676\n",
      "Computers               0.002448\n",
      "Golf Apparel            0.002443\n",
      "Hunting & Shooting      0.002437\n",
      "Music                   0.002404\n",
      "Consumer Electronics    0.002388\n",
      "Boxing & MMA            0.002343\n",
      "Books                   0.002244\n",
      "Kids' Golf Clubs        0.002127\n",
      "Health and Beauty       0.002005\n",
      "Sporting Goods          0.001978\n",
      "Lacrosse                0.001900\n",
      "Tennis & Racquet        0.001817\n",
      "Fitness Accessories     0.001712\n",
      "Men's Golf Clubs        0.001568\n",
      "CDs                     0.001501\n",
      "Men's Clothing          0.001152\n",
      "Baby                    0.001147\n",
      "Women's Golf Clubs      0.001003\n",
      "Soccer                  0.000764\n",
      "Strength Training       0.000615\n",
      "As Seen on  TV!         0.000377\n",
      "Basketball              0.000371\n",
      "Golf Bags & Carts       0.000338\n",
      "Name: proportion, dtype: float64\n",
      "Customer City\n",
      "Customer City\n",
      "Caguas         0.369878\n",
      "Chicago        0.021521\n",
      "Los Angeles    0.018929\n",
      "Brooklyn       0.018901\n",
      "New York       0.010060\n",
      "                 ...   \n",
      "Bartlett       0.000138\n",
      "Malden         0.000122\n",
      "Ponce          0.000122\n",
      "Freehold       0.000072\n",
      "CA             0.000017\n",
      "Name: proportion, Length: 563, dtype: float64\n",
      "Customer Country\n",
      "Customer Country\n",
      "EE. UU.        0.615703\n",
      "Puerto Rico    0.384297\n",
      "Name: proportion, dtype: float64\n",
      "Customer Segment\n",
      "Customer Segment\n",
      "Consumer       0.517973\n",
      "Corporate      0.303508\n",
      "Home Office    0.178519\n",
      "Name: proportion, dtype: float64\n",
      "Customer State\n",
      "Customer State\n",
      "PR       0.384297\n",
      "CA       0.161883\n",
      "NY       0.062747\n",
      "TX       0.050427\n",
      "IL       0.042273\n",
      "FL       0.030224\n",
      "OH       0.022685\n",
      "PA       0.021183\n",
      "MI       0.021073\n",
      "NJ       0.017677\n",
      "AZ       0.016763\n",
      "GA       0.013866\n",
      "MD       0.013378\n",
      "NC       0.011035\n",
      "CO       0.010603\n",
      "VA       0.010243\n",
      "OR       0.009240\n",
      "MA       0.008902\n",
      "TN       0.008764\n",
      "NV       0.007977\n",
      "MO       0.007501\n",
      "HI       0.006913\n",
      "CT       0.006060\n",
      "UT       0.005362\n",
      "NM       0.005257\n",
      "LA       0.005252\n",
      "WA       0.005096\n",
      "WI       0.004709\n",
      "MN       0.003723\n",
      "SC       0.003684\n",
      "IN       0.003218\n",
      "DC       0.003207\n",
      "KY       0.002698\n",
      "KS       0.002537\n",
      "DE       0.001490\n",
      "RI       0.001346\n",
      "WV       0.001335\n",
      "OK       0.001285\n",
      "ND       0.001191\n",
      "ID       0.000925\n",
      "AR       0.000908\n",
      "MT       0.000482\n",
      "IA       0.000371\n",
      "AL       0.000194\n",
      "95758    0.000011\n",
      "91732    0.000006\n",
      "Name: proportion, dtype: float64\n",
      "Customer Street\n",
      "Customer Street\n",
      "9126 Wishing Expressway     0.000676\n",
      "4388 Burning Goose Ridge    0.000648\n",
      "4720 Noble Hills Wynd       0.000643\n",
      "2878 Hazy Wagon  Thicket    0.000626\n",
      "398 Emerald Grove           0.000604\n",
      "                              ...   \n",
      "7676 Colonial Towers        0.000006\n",
      "6425 Thunder Impasse        0.000006\n",
      "376 Silver Terrace          0.000006\n",
      "5420 Rocky Port             0.000006\n",
      "8621 Broad Forest           0.000006\n",
      "Name: proportion, Length: 7458, dtype: float64\n",
      "Customer Zipcode\n",
      "Customer Zipcode\n",
      "725.0      0.369878\n",
      "921.0      0.001867\n",
      "23455.0    0.001850\n",
      "957.0      0.001645\n",
      "79109.0    0.001618\n",
      "             ...   \n",
      "89015.0    0.000089\n",
      "32210.0    0.000083\n",
      "7728.0     0.000072\n",
      "11225.0    0.000050\n",
      "NaN        0.000017\n",
      "Name: proportion, Length: 996, dtype: float64\n",
      "Department Id\n",
      "Department Id\n",
      "7     0.370382\n",
      "4     0.271428\n",
      "5     0.184025\n",
      "3     0.080462\n",
      "6     0.053656\n",
      "2     0.013733\n",
      "9     0.011223\n",
      "10    0.008115\n",
      "11    0.002725\n",
      "8     0.002244\n",
      "12    0.002005\n",
      "Name: proportion, dtype: float64\n",
      "Market\n",
      "Market\n",
      "LATAM           0.285809\n",
      "Europe          0.278375\n",
      "Pacific Asia    0.228563\n",
      "USCA            0.142916\n",
      "Africa          0.064337\n",
      "Name: proportion, dtype: float64\n",
      "Order City\n",
      "Order City\n",
      "Santo Domingo    0.012248\n",
      "New York City    0.012198\n",
      "Los Angeles      0.010221\n",
      "Tegucigalpa      0.009877\n",
      "Managua          0.009318\n",
      "                   ...   \n",
      "Libourne         0.000006\n",
      "Bergerac         0.000006\n",
      "Juliaca          0.000006\n",
      "Takasaki         0.000006\n",
      "Pernik           0.000006\n",
      "Name: proportion, Length: 3597, dtype: float64\n",
      "Order Country\n",
      "Order Country\n",
      "Estados Unidos       0.137603\n",
      "Francia              0.073244\n",
      "México               0.072967\n",
      "Alemania             0.052981\n",
      "Australia            0.047070\n",
      "                       ...   \n",
      "Guinea Ecuatorial    0.000011\n",
      "Kuwait               0.000011\n",
      "Sáhara Occidental    0.000011\n",
      "Burundi              0.000006\n",
      "Serbia               0.000006\n",
      "Name: proportion, Length: 164, dtype: float64\n",
      "Order Item Cardprod Id\n",
      "Order Item Cardprod Id\n",
      "365     0.135803\n",
      "403     0.123234\n",
      "502     0.116525\n",
      "1014    0.106903\n",
      "1004    0.095973\n",
      "          ...   \n",
      "127     0.000150\n",
      "208     0.000083\n",
      "860     0.000061\n",
      "226     0.000055\n",
      "60      0.000055\n",
      "Name: proportion, Length: 118, dtype: float64\n",
      "Order Item Id\n",
      "Order Item Id\n",
      "180517    0.000006\n",
      "42681     0.000006\n",
      "28953     0.000006\n",
      "39851     0.000006\n",
      "173601    0.000006\n",
      "            ...   \n",
      "14991     0.000006\n",
      "150102    0.000006\n",
      "138584    0.000006\n",
      "150270    0.000006\n",
      "65113     0.000006\n",
      "Name: proportion, Length: 180519, dtype: float64\n",
      "Order State\n",
      "Order State\n",
      "Inglaterra                     0.037237\n",
      "California                     0.027510\n",
      "Isla de Francia                0.025371\n",
      "Renania del Norte-Westfalia    0.018297\n",
      "San Salvador                   0.016923\n",
      "                                 ...   \n",
      "Kabarole                       0.000006\n",
      "Vrancea                        0.000006\n",
      "Buyumbura Mairie               0.000006\n",
      "Aomori                         0.000006\n",
      "Iringa                         0.000006\n",
      "Name: proportion, Length: 1089, dtype: float64\n",
      "Order Status\n",
      "Order Status\n",
      "COMPLETE           0.329555\n",
      "PENDING_PAYMENT    0.220653\n",
      "PROCESSING         0.121328\n",
      "PENDING            0.112049\n",
      "CLOSED             0.108664\n",
      "ON_HOLD            0.054310\n",
      "SUSPECTED_FRAUD    0.022502\n",
      "CANCELED           0.020452\n",
      "PAYMENT_REVIEW     0.010486\n",
      "Name: proportion, dtype: float64\n",
      "Order Region\n",
      "Order Region\n",
      "Central America    0.156997\n",
      "Western Europe     0.150173\n",
      "South America      0.082734\n",
      "Oceania            0.056216\n",
      "Northern Europe    0.054244\n",
      "Southeast Asia     0.052842\n",
      "Southern Europe    0.052244\n",
      "Caribbean          0.046078\n",
      "West of USA        0.044278\n",
      "South Asia         0.042827\n",
      "Eastern Asia       0.040328\n",
      "East of USA        0.038306\n",
      "West Asia          0.033287\n",
      "US Center          0.032612\n",
      "South of  USA      0.022408\n",
      "Eastern Europe     0.021715\n",
      "West Africa        0.020474\n",
      "North Africa       0.017904\n",
      "East Africa        0.010259\n",
      "Central Africa     0.009290\n",
      "Southern Africa    0.006409\n",
      "Canada             0.005312\n",
      "Central Asia       0.003063\n",
      "Name: proportion, dtype: float64\n",
      "Order Zipcode\n",
      "Order Zipcode\n",
      "NaN        0.862397\n",
      "10035.0    0.003590\n",
      "10009.0    0.003047\n",
      "10024.0    0.002997\n",
      "94122.0    0.002914\n",
      "             ...   \n",
      "63376.0    0.000006\n",
      "31088.0    0.000006\n",
      "60477.0    0.000006\n",
      "8401.0     0.000006\n",
      "61832.0    0.000006\n",
      "Name: proportion, Length: 610, dtype: float64\n",
      "Product Card Id\n",
      "Product Card Id\n",
      "365     0.135803\n",
      "403     0.123234\n",
      "502     0.116525\n",
      "1014    0.106903\n",
      "1004    0.095973\n",
      "          ...   \n",
      "127     0.000150\n",
      "208     0.000083\n",
      "860     0.000061\n",
      "226     0.000055\n",
      "60      0.000055\n",
      "Name: proportion, Length: 118, dtype: float64\n",
      "Product Category Id\n",
      "Product Category Id\n",
      "17    0.136002\n",
      "18    0.123234\n",
      "24    0.116525\n",
      "46    0.106903\n",
      "45    0.095973\n",
      "48    0.086085\n",
      "43    0.076053\n",
      "9     0.069173\n",
      "29    0.060847\n",
      "37    0.011240\n",
      "40    0.009860\n",
      "36    0.008171\n",
      "26    0.006653\n",
      "13    0.006243\n",
      "35    0.005927\n",
      "41    0.005396\n",
      "75    0.004642\n",
      "63    0.003612\n",
      "76    0.003601\n",
      "3     0.003501\n",
      "7     0.003401\n",
      "62    0.003279\n",
      "74    0.002930\n",
      "33    0.002903\n",
      "72    0.002725\n",
      "66    0.002681\n",
      "68    0.002681\n",
      "67    0.002676\n",
      "64    0.002448\n",
      "32    0.002443\n",
      "44    0.002437\n",
      "71    0.002404\n",
      "65    0.002388\n",
      "12    0.002343\n",
      "59    0.002244\n",
      "38    0.002127\n",
      "69    0.002005\n",
      "73    0.001978\n",
      "5     0.001900\n",
      "6     0.001817\n",
      "11    0.001712\n",
      "30    0.001568\n",
      "61    0.001501\n",
      "70    0.001152\n",
      "60    0.001147\n",
      "31    0.001003\n",
      "2     0.000764\n",
      "10    0.000615\n",
      "16    0.000377\n",
      "4     0.000371\n",
      "34    0.000338\n",
      "Name: proportion, dtype: float64\n",
      "Order Zipcode\n",
      "Order Zipcode\n",
      "NaN        0.862397\n",
      "10035.0    0.003590\n",
      "10009.0    0.003047\n",
      "10024.0    0.002997\n",
      "94122.0    0.002914\n",
      "             ...   \n",
      "63376.0    0.000006\n",
      "31088.0    0.000006\n",
      "60477.0    0.000006\n",
      "8401.0     0.000006\n",
      "61832.0    0.000006\n",
      "Name: proportion, Length: 610, dtype: float64\n",
      "Product Card Id\n",
      "Product Card Id\n",
      "365     0.135803\n",
      "403     0.123234\n",
      "502     0.116525\n",
      "1014    0.106903\n",
      "1004    0.095973\n",
      "          ...   \n",
      "127     0.000150\n",
      "208     0.000083\n",
      "860     0.000061\n",
      "226     0.000055\n",
      "60      0.000055\n",
      "Name: proportion, Length: 118, dtype: float64\n",
      "Product Category Id\n",
      "Product Category Id\n",
      "17    0.136002\n",
      "18    0.123234\n",
      "24    0.116525\n",
      "46    0.106903\n",
      "45    0.095973\n",
      "48    0.086085\n",
      "43    0.076053\n",
      "9     0.069173\n",
      "29    0.060847\n",
      "37    0.011240\n",
      "40    0.009860\n",
      "36    0.008171\n",
      "26    0.006653\n",
      "13    0.006243\n",
      "35    0.005927\n",
      "41    0.005396\n",
      "75    0.004642\n",
      "63    0.003612\n",
      "76    0.003601\n",
      "3     0.003501\n",
      "7     0.003401\n",
      "62    0.003279\n",
      "74    0.002930\n",
      "33    0.002903\n",
      "72    0.002725\n",
      "66    0.002681\n",
      "68    0.002681\n",
      "67    0.002676\n",
      "64    0.002448\n",
      "32    0.002443\n",
      "44    0.002437\n",
      "71    0.002404\n",
      "65    0.002388\n",
      "12    0.002343\n",
      "59    0.002244\n",
      "38    0.002127\n",
      "69    0.002005\n",
      "73    0.001978\n",
      "5     0.001900\n",
      "6     0.001817\n",
      "11    0.001712\n",
      "30    0.001568\n",
      "61    0.001501\n",
      "70    0.001152\n",
      "60    0.001147\n",
      "31    0.001003\n",
      "2     0.000764\n",
      "10    0.000615\n",
      "16    0.000377\n",
      "4     0.000371\n",
      "34    0.000338\n",
      "Name: proportion, dtype: float64\n",
      "Product Name\n",
      "Product Name\n",
      "Perfect Fitness Perfect Rip Deck                 0.135803\n",
      "Nike Men's CJ Elite 2 TD Football Cleat          0.123234\n",
      "Nike Men's Dri-FIT Victory Golf Polo             0.116525\n",
      "O'Brien Men's Neoprene Life Vest                 0.106903\n",
      "Field & Stream Sportsman 16 Gun Fire Safe        0.095973\n",
      "                                                   ...   \n",
      "Stiga Master Series ST3100 Competition Indoor    0.000150\n",
      "SOLE E35 Elliptical                              0.000083\n",
      "Bushnell Pro X7 Jolt Slope Rangefinder           0.000061\n",
      "Bowflex SelectTech 1090 Dumbbells                0.000055\n",
      "SOLE E25 Elliptical                              0.000055\n",
      "Name: proportion, Length: 118, dtype: float64\n",
      "Shipping Mode\n",
      "Shipping Mode\n",
      "Standard Class    0.596901\n",
      "Second Class      0.195082\n",
      "First Class       0.154078\n",
      "Same Day          0.053939\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days for shipment (scheduled)</th>\n",
       "      <th>Benefit per order</th>\n",
       "      <th>Sales per customer</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Order Item Discount</th>\n",
       "      <th>Order Item Discount Rate</th>\n",
       "      <th>Order Item Product Price</th>\n",
       "      <th>Order Item Profit Ratio</th>\n",
       "      <th>Order Item Quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>Order Item Cardprod Id_Frequency</th>\n",
       "      <th>Order Item Id_Frequency</th>\n",
       "      <th>Order State_Frequency</th>\n",
       "      <th>Order Status_Frequency</th>\n",
       "      <th>Order Region_Frequency</th>\n",
       "      <th>Order Zipcode_Frequency</th>\n",
       "      <th>Product Card Id_Frequency</th>\n",
       "      <th>Product Category Id_Frequency</th>\n",
       "      <th>Product Name_Frequency</th>\n",
       "      <th>Shipping Mode_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>314.640015</td>\n",
       "      <td>18.251453</td>\n",
       "      <td>-66.037056</td>\n",
       "      <td>13.110000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-249.089996</td>\n",
       "      <td>311.359985</td>\n",
       "      <td>18.279451</td>\n",
       "      <td>-66.037064</td>\n",
       "      <td>16.389999</td>\n",
       "      <td>0.05</td>\n",
       "      <td>327.75</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>-247.779999</td>\n",
       "      <td>309.720001</td>\n",
       "      <td>37.292233</td>\n",
       "      <td>-121.881279</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>0.06</td>\n",
       "      <td>327.75</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.108664</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>22.860001</td>\n",
       "      <td>304.809998</td>\n",
       "      <td>34.125946</td>\n",
       "      <td>-118.291016</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>0.07</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>134.210007</td>\n",
       "      <td>298.250000</td>\n",
       "      <td>18.253769</td>\n",
       "      <td>-66.037048</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.220653</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Days for shipment (scheduled)  Benefit per order  Sales per customer  \\\n",
       "0                              4          91.250000          314.640015   \n",
       "1                              4        -249.089996          311.359985   \n",
       "2                              4        -247.779999          309.720001   \n",
       "3                              4          22.860001          304.809998   \n",
       "4                              4         134.210007          298.250000   \n",
       "\n",
       "    Latitude   Longitude  Order Item Discount  Order Item Discount Rate  \\\n",
       "0  18.251453  -66.037056            13.110000                      0.04   \n",
       "1  18.279451  -66.037064            16.389999                      0.05   \n",
       "2  37.292233 -121.881279            18.030001                      0.06   \n",
       "3  34.125946 -118.291016            22.940001                      0.07   \n",
       "4  18.253769  -66.037048            29.500000                      0.09   \n",
       "\n",
       "   Order Item Product Price  Order Item Profit Ratio  Order Item Quantity  \\\n",
       "0                    327.75                     0.29                    1   \n",
       "1                    327.75                    -0.80                    1   \n",
       "2                    327.75                    -0.80                    1   \n",
       "3                    327.75                     0.08                    1   \n",
       "4                    327.75                     0.45                    1   \n",
       "\n",
       "   ...  Order Item Cardprod Id_Frequency  Order Item Id_Frequency  \\\n",
       "0  ...                          0.001978                 0.000006   \n",
       "1  ...                          0.001978                 0.000006   \n",
       "2  ...                          0.001978                 0.000006   \n",
       "3  ...                          0.001978                 0.000006   \n",
       "4  ...                          0.001978                 0.000006   \n",
       "\n",
       "   Order State_Frequency  Order Status_Frequency  Order Region_Frequency  \\\n",
       "0               0.004526                0.329555                0.052842   \n",
       "1               0.001728                0.112049                0.042827   \n",
       "2               0.001728                0.108664                0.042827   \n",
       "3               0.012110                0.329555                0.056216   \n",
       "4               0.012110                0.220653                0.056216   \n",
       "\n",
       "   Order Zipcode_Frequency  Product Card Id_Frequency  \\\n",
       "0                 0.862397                   0.001978   \n",
       "1                 0.862397                   0.001978   \n",
       "2                 0.862397                   0.001978   \n",
       "3                 0.862397                   0.001978   \n",
       "4                 0.862397                   0.001978   \n",
       "\n",
       "   Product Category Id_Frequency  Product Name_Frequency  \\\n",
       "0                       0.001978                0.001978   \n",
       "1                       0.001978                0.001978   \n",
       "2                       0.001978                0.001978   \n",
       "3                       0.001978                0.001978   \n",
       "4                       0.001978                0.001978   \n",
       "\n",
       "   Shipping Mode_Frequency  \n",
       "0                 0.596901  \n",
       "1                 0.596901  \n",
       "2                 0.596901  \n",
       "3                 0.596901  \n",
       "4                 0.596901  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Chọn các cột làm đặc trưng (X)\n",
    "features_columns = ['Type', 'Days for shipment (scheduled)',\n",
    "                     'Benefit per order', 'Sales per customer', 'Delivery Status',\n",
    "                    'Category Id', 'Category Name', 'Customer City',\n",
    "                     'Customer Country', 'Customer Segment',\n",
    "                     'Customer State', 'Customer Street', 'Customer Zipcode',\n",
    "                     'Department Id', 'Latitude', 'Longitude', 'Market',\n",
    "                     'Order City', 'Order Country','Order Item Cardprod Id',\n",
    "                     'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id',\n",
    "                     'Order Item Product Price', 'Order Item Profit Ratio',\n",
    "                     'Order Item Quantity', 'Sales', 'Order Item Total',\n",
    "                     'Order Profit Per Order', 'Order Region', 'Order State', 'Order Status',\n",
    "                     'Order Zipcode', 'Product Card Id', 'Product Category Id',\n",
    "                     'Product Name', 'Product Price', 'Shipping Mode',\n",
    "                     'order_year', 'order_month', 'order_day_of_week',\n",
    "                        'ship_year', 'ship_month', 'ship_day_of_week',\n",
    "                     'Late_delivery_risk', 'Days for shipping (real)']\n",
    "\n",
    "# Sao chép DataFrame để tránh ảnh hưởng đến dữ liệu gốc\n",
    "X = df.copy()\n",
    "\n",
    "# Tạo danh sách các cột cần thực hiện one-hot encoding\n",
    "categorical_columns = ['Type', 'Delivery Status', 'Category Id', 'Category Name', 'Customer City',\n",
    "                         'Customer Country', 'Customer Segment',\n",
    "                        'Customer State', 'Customer Street', 'Customer Zipcode',\n",
    "                         'Department Id', 'Market', 'Order City', 'Order Country',\n",
    "                         'Order Item Cardprod Id','Order Item Id',\n",
    "                         'Order State', 'Order Status', 'Order Region',\n",
    "                        'Order Zipcode', 'Product Card Id', 'Product Category Id',\n",
    "                        'Order Zipcode', 'Product Card Id', 'Product Category Id',\n",
    "                        'Product Name', 'Shipping Mode'\n",
    "                        ]\n",
    "\n",
    "# Thực hiện frequency encoding theo tần số normalize cho từng cột phân loại\n",
    "for column in categorical_columns:\n",
    "    # Tính tần số normalize của mỗi giá trị\n",
    "    print(column)\n",
    "    value_counts = X[column].value_counts(dropna=False, normalize=True)\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Tạo các cột mới với tên là 'TênCot_Frequency' và giá trị tương ứng\n",
    "    new_columns = X[column].map(value_counts)\n",
    "    X[column + '_Frequency'] = new_columns\n",
    "\n",
    "# Loại bỏ các cột phân loại gốc\n",
    "X = X.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Hiển thị 5 dòng đầu tiên của DataFrame kết quả\n",
    "X.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dc95cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days for shipment (scheduled)</th>\n",
       "      <th>Benefit per order</th>\n",
       "      <th>Sales per customer</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Order Item Discount</th>\n",
       "      <th>Order Item Discount Rate</th>\n",
       "      <th>Order Item Product Price</th>\n",
       "      <th>Order Item Profit Ratio</th>\n",
       "      <th>Order Item Quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>Order Item Cardprod Id_Frequency</th>\n",
       "      <th>Order Item Id_Frequency</th>\n",
       "      <th>Order State_Frequency</th>\n",
       "      <th>Order Status_Frequency</th>\n",
       "      <th>Order Region_Frequency</th>\n",
       "      <th>Order Zipcode_Frequency</th>\n",
       "      <th>Product Card Id_Frequency</th>\n",
       "      <th>Product Category Id_Frequency</th>\n",
       "      <th>Product Name_Frequency</th>\n",
       "      <th>Shipping Mode_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.158939</td>\n",
       "      <td>0.630916</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.02622</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.935385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.776183</td>\n",
       "      <td>0.157242</td>\n",
       "      <td>0.631254</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.03278</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.776435</td>\n",
       "      <td>0.156393</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.132258</td>\n",
       "      <td>0.03606</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.108664</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.828614</td>\n",
       "      <td>0.153853</td>\n",
       "      <td>0.822823</td>\n",
       "      <td>0.145395</td>\n",
       "      <td>0.04588</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.850082</td>\n",
       "      <td>0.150458</td>\n",
       "      <td>0.630944</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.05900</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.220653</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.827789</td>\n",
       "      <td>0.148766</td>\n",
       "      <td>0.930271</td>\n",
       "      <td>0.289609</td>\n",
       "      <td>0.06556</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.864615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.842557</td>\n",
       "      <td>0.145371</td>\n",
       "      <td>0.630808</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.07866</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.947692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.143674</td>\n",
       "      <td>0.723728</td>\n",
       "      <td>0.284911</td>\n",
       "      <td>0.08522</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.121328</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.849988</td>\n",
       "      <td>0.140285</td>\n",
       "      <td>0.630695</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.108664</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.849685</td>\n",
       "      <td>0.138587</td>\n",
       "      <td>0.866924</td>\n",
       "      <td>0.131946</td>\n",
       "      <td>0.10488</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.108664</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.849382</td>\n",
       "      <td>0.136890</td>\n",
       "      <td>0.631295</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.11144</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.833016</td>\n",
       "      <td>0.135198</td>\n",
       "      <td>0.901751</td>\n",
       "      <td>0.308973</td>\n",
       "      <td>0.11800</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.898462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.828402</td>\n",
       "      <td>0.131803</td>\n",
       "      <td>0.853655</td>\n",
       "      <td>0.133081</td>\n",
       "      <td>0.13110</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.828946</td>\n",
       "      <td>0.123322</td>\n",
       "      <td>0.631242</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.16388</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.827367</td>\n",
       "      <td>0.165723</td>\n",
       "      <td>0.924375</td>\n",
       "      <td>0.318706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.121328</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.774160</td>\n",
       "      <td>0.164026</td>\n",
       "      <td>0.631310</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.00656</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.776709</td>\n",
       "      <td>0.162334</td>\n",
       "      <td>0.632660</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.01312</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.609231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.220653</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.828803</td>\n",
       "      <td>0.160636</td>\n",
       "      <td>0.820076</td>\n",
       "      <td>0.145822</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.108664</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.843922</td>\n",
       "      <td>0.158939</td>\n",
       "      <td>0.630702</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.02622</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.947692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.154078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.841015</td>\n",
       "      <td>0.157242</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.256376</td>\n",
       "      <td>0.03278</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.932308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.220653</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.053939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.854064</td>\n",
       "      <td>0.156393</td>\n",
       "      <td>0.818671</td>\n",
       "      <td>0.145757</td>\n",
       "      <td>0.03606</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.121328</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.053939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.840074</td>\n",
       "      <td>0.153853</td>\n",
       "      <td>0.631301</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.04588</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.929231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.042827</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.828520</td>\n",
       "      <td>0.150458</td>\n",
       "      <td>0.877052</td>\n",
       "      <td>0.134448</td>\n",
       "      <td>0.05900</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.827619</td>\n",
       "      <td>0.148766</td>\n",
       "      <td>0.630382</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.06556</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.864615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.841613</td>\n",
       "      <td>0.145371</td>\n",
       "      <td>0.963810</td>\n",
       "      <td>0.181005</td>\n",
       "      <td>0.07866</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.941538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.849496</td>\n",
       "      <td>0.143674</td>\n",
       "      <td>0.631039</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.08522</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.841663</td>\n",
       "      <td>0.140285</td>\n",
       "      <td>0.908770</td>\n",
       "      <td>0.300620</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.947692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.056216</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.195082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.840130</td>\n",
       "      <td>0.138587</td>\n",
       "      <td>0.631142</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.10488</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.220653</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.820902</td>\n",
       "      <td>0.136890</td>\n",
       "      <td>0.916428</td>\n",
       "      <td>0.268772</td>\n",
       "      <td>0.11144</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.827692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.329555</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.828767</td>\n",
       "      <td>0.135198</td>\n",
       "      <td>0.820205</td>\n",
       "      <td>0.146002</td>\n",
       "      <td>0.11800</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.159678</td>\n",
       "      <td>0.873846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.121328</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.596901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Days for shipment (scheduled)  Benefit per order  Sales per customer  \\\n",
       "0                            1.00           0.841800            0.158939   \n",
       "1                            1.00           0.776183            0.157242   \n",
       "2                            1.00           0.776435            0.156393   \n",
       "3                            1.00           0.828614            0.153853   \n",
       "4                            1.00           0.850082            0.150458   \n",
       "5                            1.00           0.827789            0.148766   \n",
       "6                            0.25           0.842557            0.145371   \n",
       "7                            0.25           0.837400            0.143674   \n",
       "8                            0.50           0.849988            0.140285   \n",
       "9                            0.25           0.849685            0.138587   \n",
       "10                           0.50           0.849382            0.136890   \n",
       "11                           0.50           0.833016            0.135198   \n",
       "12                           0.50           0.828402            0.131803   \n",
       "13                           0.25           0.828946            0.123322   \n",
       "14                           0.25           0.827367            0.165723   \n",
       "15                           0.25           0.774160            0.164026   \n",
       "16                           0.50           0.776709            0.162334   \n",
       "17                           0.25           0.828803            0.160636   \n",
       "18                           0.25           0.843922            0.158939   \n",
       "19                           0.00           0.841015            0.157242   \n",
       "20                           0.00           0.854064            0.156393   \n",
       "21                           1.00           0.840074            0.153853   \n",
       "22                           0.50           0.828520            0.150458   \n",
       "23                           0.50           0.827619            0.148766   \n",
       "24                           0.50           0.841613            0.145371   \n",
       "25                           0.50           0.849496            0.143674   \n",
       "26                           0.50           0.841663            0.140285   \n",
       "27                           1.00           0.840130            0.138587   \n",
       "28                           1.00           0.820902            0.136890   \n",
       "29                           1.00           0.828767            0.135198   \n",
       "\n",
       "    Latitude  Longitude  Order Item Discount  Order Item Discount Rate  \\\n",
       "0   0.630916   0.336599              0.02622                      0.16   \n",
       "1   0.631254   0.336599              0.03278                      0.20   \n",
       "2   0.861100   0.132258              0.03606                      0.24   \n",
       "3   0.822823   0.145395              0.04588                      0.28   \n",
       "4   0.630944   0.336599              0.05900                      0.36   \n",
       "5   0.930271   0.289609              0.06556                      0.40   \n",
       "6   0.630808   0.336599              0.07866                      0.48   \n",
       "7   0.723728   0.284911              0.08522                      0.52   \n",
       "8   0.630695   0.336599              0.09832                      0.60   \n",
       "9   0.866924   0.131946              0.10488                      0.64   \n",
       "10  0.631295   0.336599              0.11144                      0.68   \n",
       "11  0.901751   0.308973              0.11800                      0.72   \n",
       "12  0.853655   0.133081              0.13110                      0.80   \n",
       "13  0.631242   0.336599              0.16388                      1.00   \n",
       "14  0.924375   0.318706              0.00000                      0.00   \n",
       "15  0.631310   0.336599              0.00656                      0.04   \n",
       "16  0.632660   0.337200              0.01312                      0.08   \n",
       "17  0.820076   0.145822              0.01966                      0.12   \n",
       "18  0.630702   0.336599              0.02622                      0.16   \n",
       "19  0.918597   0.256376              0.03278                      0.20   \n",
       "20  0.818671   0.145757              0.03606                      0.24   \n",
       "21  0.631301   0.336599              0.04588                      0.28   \n",
       "22  0.877052   0.134448              0.05900                      0.36   \n",
       "23  0.630382   0.336599              0.06556                      0.40   \n",
       "24  0.963810   0.181005              0.07866                      0.48   \n",
       "25  0.631039   0.336599              0.08522                      0.52   \n",
       "26  0.908770   0.300620              0.09832                      0.60   \n",
       "27  0.631142   0.336599              0.10488                      0.64   \n",
       "28  0.916428   0.268772              0.11144                      0.68   \n",
       "29  0.820205   0.146002              0.11800                      0.72   \n",
       "\n",
       "    Order Item Product Price  Order Item Profit Ratio  Order Item Quantity  \\\n",
       "0                   0.159678                 0.935385                  0.0   \n",
       "1                   0.159678                 0.600000                  0.0   \n",
       "2                   0.159678                 0.600000                  0.0   \n",
       "3                   0.159678                 0.870769                  0.0   \n",
       "4                   0.159678                 0.984615                  0.0   \n",
       "5                   0.159678                 0.864615                  0.0   \n",
       "6                   0.159678                 0.947692                  0.0   \n",
       "7                   0.159678                 0.920000                  0.0   \n",
       "8                   0.159678                 0.993846                  0.0   \n",
       "9                   0.159678                 0.993846                  0.0   \n",
       "10                  0.159678                 0.993846                  0.0   \n",
       "11                  0.159678                 0.898462                  0.0   \n",
       "12                  0.159678                 0.870769                  0.0   \n",
       "13                  0.159678                 0.876923                  0.0   \n",
       "14                  0.159678                 0.861538                  0.0   \n",
       "15                  0.159678                 0.600000                  0.0   \n",
       "16                  0.159678                 0.609231                  0.0   \n",
       "17                  0.159678                 0.870769                  0.0   \n",
       "18                  0.159678                 0.947692                  0.0   \n",
       "19                  0.159678                 0.932308                  0.0   \n",
       "20                  0.159678                 1.000000                  0.0   \n",
       "21                  0.159678                 0.929231                  0.0   \n",
       "22                  0.159678                 0.870769                  0.0   \n",
       "23                  0.159678                 0.864615                  0.0   \n",
       "24                  0.159678                 0.941538                  0.0   \n",
       "25                  0.159678                 0.987692                  0.0   \n",
       "26                  0.159678                 0.947692                  0.0   \n",
       "27                  0.159678                 0.938462                  0.0   \n",
       "28                  0.159678                 0.827692                  0.0   \n",
       "29                  0.159678                 0.873846                  0.0   \n",
       "\n",
       "    ...  Order Item Cardprod Id_Frequency  Order Item Id_Frequency  \\\n",
       "0   ...                          0.001978                 0.000006   \n",
       "1   ...                          0.001978                 0.000006   \n",
       "2   ...                          0.001978                 0.000006   \n",
       "3   ...                          0.001978                 0.000006   \n",
       "4   ...                          0.001978                 0.000006   \n",
       "5   ...                          0.001978                 0.000006   \n",
       "6   ...                          0.001978                 0.000006   \n",
       "7   ...                          0.001978                 0.000006   \n",
       "8   ...                          0.001978                 0.000006   \n",
       "9   ...                          0.001978                 0.000006   \n",
       "10  ...                          0.001978                 0.000006   \n",
       "11  ...                          0.001978                 0.000006   \n",
       "12  ...                          0.001978                 0.000006   \n",
       "13  ...                          0.001978                 0.000006   \n",
       "14  ...                          0.001978                 0.000006   \n",
       "15  ...                          0.001978                 0.000006   \n",
       "16  ...                          0.001978                 0.000006   \n",
       "17  ...                          0.001978                 0.000006   \n",
       "18  ...                          0.001978                 0.000006   \n",
       "19  ...                          0.001978                 0.000006   \n",
       "20  ...                          0.001978                 0.000006   \n",
       "21  ...                          0.001978                 0.000006   \n",
       "22  ...                          0.001978                 0.000006   \n",
       "23  ...                          0.001978                 0.000006   \n",
       "24  ...                          0.001978                 0.000006   \n",
       "25  ...                          0.001978                 0.000006   \n",
       "26  ...                          0.001978                 0.000006   \n",
       "27  ...                          0.001978                 0.000006   \n",
       "28  ...                          0.001978                 0.000006   \n",
       "29  ...                          0.001978                 0.000006   \n",
       "\n",
       "    Order State_Frequency  Order Status_Frequency  Order Region_Frequency  \\\n",
       "0                0.004526                0.329555                0.052842   \n",
       "1                0.001728                0.112049                0.042827   \n",
       "2                0.001728                0.108664                0.042827   \n",
       "3                0.012110                0.329555                0.056216   \n",
       "4                0.012110                0.220653                0.056216   \n",
       "5                0.012110                0.020452                0.056216   \n",
       "6                0.004747                0.329555                0.040328   \n",
       "7                0.004747                0.121328                0.040328   \n",
       "8                0.004747                0.108664                0.040328   \n",
       "9                0.004747                0.108664                0.040328   \n",
       "10               0.000997                0.022502                0.040328   \n",
       "11               0.000249                0.112049                0.052842   \n",
       "12               0.000249                0.112049                0.052842   \n",
       "13               0.005013                0.329555                0.042827   \n",
       "14               0.005013                0.121328                0.042827   \n",
       "15               0.005013                0.329555                0.042827   \n",
       "16               0.001490                0.220653                0.040328   \n",
       "17               0.001374                0.108664                0.042827   \n",
       "18               0.001374                0.329555                0.042827   \n",
       "19               0.001374                0.220653                0.042827   \n",
       "20               0.001374                0.121328                0.042827   \n",
       "21               0.001374                0.112049                0.042827   \n",
       "22               0.008066                0.112049                0.056216   \n",
       "23               0.008066                0.020452                0.056216   \n",
       "24               0.012110                0.022502                0.056216   \n",
       "25               0.005080                0.329555                0.056216   \n",
       "26               0.005080                0.112049                0.056216   \n",
       "27               0.000537                0.220653                0.040328   \n",
       "28               0.000537                0.329555                0.040328   \n",
       "29               0.000537                0.121328                0.040328   \n",
       "\n",
       "    Order Zipcode_Frequency  Product Card Id_Frequency  \\\n",
       "0                  0.862397                   0.001978   \n",
       "1                  0.862397                   0.001978   \n",
       "2                  0.862397                   0.001978   \n",
       "3                  0.862397                   0.001978   \n",
       "4                  0.862397                   0.001978   \n",
       "5                  0.862397                   0.001978   \n",
       "6                  0.862397                   0.001978   \n",
       "7                  0.862397                   0.001978   \n",
       "8                  0.862397                   0.001978   \n",
       "9                  0.862397                   0.001978   \n",
       "10                 0.862397                   0.001978   \n",
       "11                 0.862397                   0.001978   \n",
       "12                 0.862397                   0.001978   \n",
       "13                 0.862397                   0.001978   \n",
       "14                 0.862397                   0.001978   \n",
       "15                 0.862397                   0.001978   \n",
       "16                 0.862397                   0.001978   \n",
       "17                 0.862397                   0.001978   \n",
       "18                 0.862397                   0.001978   \n",
       "19                 0.862397                   0.001978   \n",
       "20                 0.862397                   0.001978   \n",
       "21                 0.862397                   0.001978   \n",
       "22                 0.862397                   0.001978   \n",
       "23                 0.862397                   0.001978   \n",
       "24                 0.862397                   0.001978   \n",
       "25                 0.862397                   0.001978   \n",
       "26                 0.862397                   0.001978   \n",
       "27                 0.862397                   0.001978   \n",
       "28                 0.862397                   0.001978   \n",
       "29                 0.862397                   0.001978   \n",
       "\n",
       "    Product Category Id_Frequency  Product Name_Frequency  \\\n",
       "0                        0.001978                0.001978   \n",
       "1                        0.001978                0.001978   \n",
       "2                        0.001978                0.001978   \n",
       "3                        0.001978                0.001978   \n",
       "4                        0.001978                0.001978   \n",
       "5                        0.001978                0.001978   \n",
       "6                        0.001978                0.001978   \n",
       "7                        0.001978                0.001978   \n",
       "8                        0.001978                0.001978   \n",
       "9                        0.001978                0.001978   \n",
       "10                       0.001978                0.001978   \n",
       "11                       0.001978                0.001978   \n",
       "12                       0.001978                0.001978   \n",
       "13                       0.001978                0.001978   \n",
       "14                       0.001978                0.001978   \n",
       "15                       0.001978                0.001978   \n",
       "16                       0.001978                0.001978   \n",
       "17                       0.001978                0.001978   \n",
       "18                       0.001978                0.001978   \n",
       "19                       0.001978                0.001978   \n",
       "20                       0.001978                0.001978   \n",
       "21                       0.001978                0.001978   \n",
       "22                       0.001978                0.001978   \n",
       "23                       0.001978                0.001978   \n",
       "24                       0.001978                0.001978   \n",
       "25                       0.001978                0.001978   \n",
       "26                       0.001978                0.001978   \n",
       "27                       0.001978                0.001978   \n",
       "28                       0.001978                0.001978   \n",
       "29                       0.001978                0.001978   \n",
       "\n",
       "    Shipping Mode_Frequency  \n",
       "0                  0.596901  \n",
       "1                  0.596901  \n",
       "2                  0.596901  \n",
       "3                  0.596901  \n",
       "4                  0.596901  \n",
       "5                  0.596901  \n",
       "6                  0.154078  \n",
       "7                  0.154078  \n",
       "8                  0.195082  \n",
       "9                  0.154078  \n",
       "10                 0.195082  \n",
       "11                 0.195082  \n",
       "12                 0.195082  \n",
       "13                 0.154078  \n",
       "14                 0.154078  \n",
       "15                 0.154078  \n",
       "16                 0.195082  \n",
       "17                 0.154078  \n",
       "18                 0.154078  \n",
       "19                 0.053939  \n",
       "20                 0.053939  \n",
       "21                 0.596901  \n",
       "22                 0.195082  \n",
       "23                 0.195082  \n",
       "24                 0.195082  \n",
       "25                 0.195082  \n",
       "26                 0.195082  \n",
       "27                 0.596901  \n",
       "28                 0.596901  \n",
       "29                 0.596901  \n",
       "\n",
       "[30 rows x 46 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_normalize = ['Days for shipment (scheduled)', 'Benefit per order', 'Sales per customer',\n",
    "                        'Latitude', 'Longitude','Order Item Discount', 'Order Item Discount Rate',\n",
    "                        'Order Item Product Price', 'Order Item Profit Ratio',\n",
    "                        'Order Item Quantity', 'Sales', 'Order Item Total',\n",
    "                        'Order Profit Per Order', 'Product Price', 'Late_delivery_risk', 'Days for shipping (real)']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X[columns_to_normalize] = scaler.fit_transform(X[columns_to_normalize])\n",
    "\n",
    "X.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc3fb40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180519, 46)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280d55a",
   "metadata": {},
   "source": [
    "FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1469bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = ['Days for shipping (real)']\n",
    "features = X.drop(target_variable, axis=1)\n",
    "target = X[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, mean_squared_error  # Consider using MSE for continuous target variables\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def feature_selection(X, target_variable, method=\"chi2\", k=10):\n",
    "\n",
    "#   # Check if target variable exists\n",
    "#   if target_variable not in X.columns:\n",
    "#     raise ValueError(f\"Target variable '{target_variable}' not found in DataFrame.\")\n",
    "\n",
    "#   # Separate features and target variable\n",
    "#   features = X.drop(target_variable, axis=1)\n",
    "#   target = X[target_variable]\n",
    "\n",
    "#   # Perform train-test split on features and target combined\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(pd.concat([features, target], axis=1), target, test_size=0.2, random_state=42)\n",
    "\n",
    "#   # No need for target encoding since assumed numerical\n",
    "\n",
    "#   # Select features based on chosen method\n",
    "#   if method == \"chi2\":\n",
    "#     selector = SelectKBest(chi2, k=k)\n",
    "#   elif method == \"f_classif\":\n",
    "#     selector = SelectKBest(f_classif, k=k)\n",
    "#   else:\n",
    "#     raise ValueError(f\"Invalid feature selection method: {method}\")\n",
    "\n",
    "#   # Fit the selector and get selected features\n",
    "#   selector.fit(X_train.drop(target_variable, axis=1), y_train)\n",
    "#   X_train_selected = selector.transform(X_train.drop(target_variable, axis=1))\n",
    "#   X_test_selected = selector.transform(X_test.drop(target_variable, axis=1))\n",
    "\n",
    "#   return X_train_selected, X_test_selected\n",
    "\n",
    "# # Split data into training and testing sets (using feature_selection)\n",
    "# X_train, X_test = feature_selection(X.copy(), target_variable)\n",
    "\n",
    "# # List to store performance metrics and feature sets\n",
    "# performance_scores = []\n",
    "# best_features = None\n",
    "# best_score = 0\n",
    "\n",
    "# # Loop through various numbers of features\n",
    "# for k in range(1, len(X.columns) - 1):  # Adjust loop range to exclude target variable\n",
    "#   # Perform feature selection with k features\n",
    "#   X_train_selected, X_test_selected = feature_selection(X_train.copy(), target_variable, k=k)\n",
    "\n",
    "#   # Train model on selected features (consider model selection based on your data)\n",
    "#   model = RandomForestRegressor()  # Adjust model type if 'Delivery_Time' is categorical\n",
    "#   model.fit(X_train_selected, X_train[target_variable])\n",
    "\n",
    "#   # Evaluate model on testing set\n",
    "#   y_pred = model.predict(X_test_selected)\n",
    "#   mse = mean_squared_error(X_test[target_variable], y_pred)\n",
    "\n",
    "#   # Store performance scores (MSE)\n",
    "#   performance_scores.append(mse)\n",
    "\n",
    "#   # Optionally, print the performance scores for each iteration\n",
    "#   print(f\"Number of features: {k}\")\n",
    "#   print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "#   print(\"-\" * 30)  # Separator for readability\n",
    "\n",
    "#   # Identify best performing model (optional)\n",
    "#   if mse < best_score:\n",
    "#     best_score = mse\n",
    "#     best_features = X_train.columns[X_train.columns != target_variable][:k]  # Get selected feature names\n",
    "\n",
    "# # Print best performing model details (optional)\n",
    "# print(\"Best number of features:\", len(best_features))\n",
    "# print(\"Best features:\", best_features.tolist())\n",
    "# print(\"Best MSE:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00165c7f",
   "metadata": {},
   "source": [
    "BOOSTING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65aa4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DPhuc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe904bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9cfb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_variables = ['Days for shipping (real)']\n",
    "features = X.drop('Days for shipping (real)', axis=1)\n",
    "target = X['Days for shipping (real)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8eff3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost:0.815:76.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit AdaBoost model\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'AdaBoost': adaboost_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'AdaBoost:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'AdaBoost:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "735eecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting:0.826:49.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit Gradient Boosting model\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'GradientBoosting': gradientboost_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'GradientBoosting:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'GradientBoosting:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f718a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:0.911:4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGboost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit XGBoost model\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'XGBoost': xgboost_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'XGBoost:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'XGBoost:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "770b462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "LightGBM:0.88:3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit LightGBM model\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'LightGBM': lightgbm_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'LightGBM:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'LightGBM:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21c2407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost:0.864:2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CatBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit CatBoost model\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'CatBoost': catboost_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'CatBoost:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'CatBoost:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cde415a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoost:0.871:5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit HistGradientBoosting model\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_fold = pd.DataFrame({'HistGradientBoost': histgradientboost_predictions})\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'HistGradientBoost:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'HistGradientBoost:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost LightGBM CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost LightGBM CatBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3  AdaBoost  GradientBoosting  XGBoost  LightGBM  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost LightGBM HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 AdaBoost  GradientBoosting  XGBoost  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17193a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5  AdaBoost  GradientBoosting  LightGBM  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting LightGBM CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e32cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 AdaBoost  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost XGBoost LightGBM CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7663e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 GradientBoosting  XGBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"GradientBoosting XGBoost LightGBM CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630855f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8  AdaBoost  GradientBoosting  XGBoost  LightGBM \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost LightGBM\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 AdaBoost  GradientBoosting  XGBoost  CatBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost CatBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf34543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 AdaBoost  GradientBoosting  XGBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting XGBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962225f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 AdaBoost  GradientBoosting  LightGBM  CatBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting LightGBM CatBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 AdaBoost  GradientBoosting  LightGBM  HistGradientBoost\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting LightGBM HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78267534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 AdaBoost  GradientBoosting  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    gradientboost_model = GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "    gradientboost_model.fit(X_train, y_train)\n",
    "    gradientboost_predictions = gradientboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'GradientBoosting': gradientboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost GradientBoosting CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 AdaBoost  XGBoost  LightGBM  CatBoost\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost XGBoost LightGBM CatBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249407f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 AdaBoost  XGBoost  LightGBM  HistGradientBoost\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost XGBoost LightGBM HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59132e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16  AdaBoost  XGBoost  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost XGBoost CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1392583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 AdaBoost  LightGBM  CatBoost  HistGradientBoost \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize K-fold\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    adaboost_model = AdaBoostRegressor(n_estimators=10, random_state=42)\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'AdaBoost': adaboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Standardize meta-features\n",
    "scaler = StandardScaler()\n",
    "meta_features_all_scaled = scaler.fit_transform(meta_features_all)\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_scaled, true_labels_all)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_scaled)\n",
    "\n",
    "# Evaluate the performance of the meta-model using R2\n",
    "r2_meta = r2_score(true_labels_all, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"AdaBoost LightGBM CatBoost HistGradientBoost\"\n",
    "\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}'\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62d65343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "XGBoost LightGBM:0.915:5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost LighGBM\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost LightGBM\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2943d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost CatBoost:0.919:3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost CatBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost CatBoost\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52e0fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost HistGradientBoost:0.918:7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost HistGradientBoost\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d722815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "LightGBM CatBoost:0.888:4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM CatBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    \n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'CatBoost': catboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"LightGBM CatBoost\"\n",
    "\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70904916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "LightGBM HistGradientBoost:0.891:8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    \n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"LightGBM HistGradientBoost\"\n",
    "\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2b3156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost HistGradientBoost:0.871:6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoost HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    \n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"CatBoost HistGradientBoost\"\n",
    "\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c42ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "XGBoost CatBoost LightGBM:0.921:6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost CatBoost LightGBM\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost CatBoost LightGBM\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "725f9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost CatBoost HistGradientBoost:0.919:8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost CatBoost HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost CatBoost HistGradientBoost\"\n",
    "\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0794a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "XGBoost LightGBM HistGradientBoost:Time: 8.16 seconds, R2-score: 0.9215, MSE: 0.0058, MAE: 0.0448, Adjusted R2: 0.9215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost LightGBM HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost': xgboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost LightGBM HistGradientBoost\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{result_str}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b005bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "CatBoost LightGBM HistGradientBoost:0.893:8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoost LightGBM HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"CatBoost LightGBM HistGradientBoost\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23a2b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582928\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582813\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4164\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.582309\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144415, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583055\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4167\n",
      "[LightGBM] [Info] Number of data points in the train set: 144416, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 0.583607\n",
      "XGBoost CatBoost LightGBM HistGradientBoost:0.923:10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoost LightGBM HistGradientBoost\n",
    "\n",
    "# Create empty arrays to store meta-features and true labels\n",
    "meta_features_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over K folds\n",
    "for train_index, test_index in kf.split(features, target):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Fit individual models with controlled complexity\n",
    "    xgboost_model = XGBRegressor(n_estimators=10, random_state=42)\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    xgboost_predictions = xgboost_model.predict(X_test)\n",
    "\n",
    "    catboost_model = CatBoostRegressor(n_estimators=10, random_state=42, verbose=False)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    catboost_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "    lightgbm_model = LGBMRegressor(n_estimators=10, random_state=42)\n",
    "    lightgbm_model.fit(X_train, y_train)\n",
    "    lightgbm_predictions = lightgbm_model.predict(X_test)\n",
    "\n",
    "    histgradientboost_model = HistGradientBoostingRegressor(max_iter=10, random_state=42)\n",
    "    histgradientboost_model.fit(X_train, y_train)\n",
    "    histgradientboost_predictions = histgradientboost_model.predict(X_test)\n",
    "\n",
    "    # Combine predictions (Stacking)\n",
    "    meta_features_fold = pd.DataFrame({\n",
    "        'XGBoost' : xgboost_predictions,\n",
    "        'CatBoost': catboost_predictions,\n",
    "        'LightGBM': lightgbm_predictions,\n",
    "        'HistGradientBoost': histgradientboost_predictions\n",
    "    })\n",
    "\n",
    "    # Append meta-features and true labels\n",
    "    meta_features_list.append(meta_features_fold)\n",
    "    true_labels_list.append(y_test)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Concatenate meta-features and true labels from all folds\n",
    "meta_features_all = pd.concat(meta_features_list, ignore_index=True)\n",
    "true_labels_all = pd.concat(true_labels_list, ignore_index=True)\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = stats.zscore(meta_features_all)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).all(axis=1)\n",
    "meta_features_all_no_outliers = meta_features_all[~outliers]\n",
    "true_labels_all_no_outliers = true_labels_all[~outliers]\n",
    "\n",
    "# Train the meta-model using Ridge Regression with L2 regularization\n",
    "alphas = [0.1, 1.0, 10.0]  # Adjust alpha values as needed\n",
    "lr_meta_model = RidgeCV(alphas=alphas)\n",
    "lr_meta_model.fit(meta_features_all_no_outliers, true_labels_all_no_outliers)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "meta_predictions = lr_meta_model.predict(meta_features_all_no_outliers)\n",
    "\n",
    "# Evaluate the performance of the meta-model\n",
    "r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "mse_meta = mean_squared_error(true_labels_all_no_outliers, meta_predictions)\n",
    "mae_meta = mean_absolute_error(true_labels_all_no_outliers, meta_predictions)\n",
    "adjusted_r2_meta = r2_score(true_labels_all_no_outliers, meta_predictions)\n",
    "\n",
    "# Print and append results to a file\n",
    "result_str = f'Time: {end_time - start_time:.2f} seconds, R2-score: {r2_meta:.4f}, MSE: {mse_meta:.4f}, MAE: {mae_meta:.4f}, Adjusted R2: {adjusted_r2_meta:.4f}'\n",
    "\n",
    "# Print and append results to a file\n",
    "algos = \"XGBoost CatBoost LightGBM HistGradientBoost\"\n",
    "\n",
    "# Append results to the specified file\n",
    "file_name = 'ensemble_dataco-supply-chain_regression_with_metrics_time.txt'\n",
    "formatted_r2 = round(r2_meta, 3)\n",
    "formatted_time = round(end_time - start_time, 0)\n",
    "\n",
    "print(f'{algos}:{formatted_r2}:{formatted_time}\\n')\n",
    "\n",
    "with open(file_name, 'a') as file:\n",
    "    file.write(f'{algos}:{result_str}\\n')\n",
    "    file.write('------------------------------------------------------------------------\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
